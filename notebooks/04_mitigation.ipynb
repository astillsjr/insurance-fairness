{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "15b79db5",
   "metadata": {},
   "source": [
    "# 04: Fairness Mitigation\n",
    "\n",
    "This notebook applies fairness-aware techniques to reduce bias:\n",
    "- **Post-processing**: Threshold optimization per protected group\n",
    "- **In-processing**: ExponentiatedGradient (constraint-based fair algorithm)\n",
    "- **Preprocessing**: Sample reweighting\n",
    "- Compare fairness-accuracy tradeoffs\n",
    "- Save mitigated models for final comparison\n",
    "\n",
    "**Approach:** We apply multiple mitigation techniques to the baseline models and evaluate their effectiveness in reducing bias while maintaining accuracy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6bc2030",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if running in Google Colab\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "except ImportError:\n",
    "    IN_COLAB = False\n",
    "\n",
    "# Install packages (for Google Colab)\n",
    "if IN_COLAB:\n",
    "    !pip install scikit-learn matplotlib seaborn fairlearn -q\n",
    "\n",
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import json\n",
    "import time\n",
    "from pathlib import Path\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score, \n",
    "    roc_auc_score\n",
    ")\n",
    "from fairlearn.metrics import (\n",
    "    MetricFrame, demographic_parity_difference, demographic_parity_ratio,\n",
    "    equalized_odds_difference, equalized_odds_ratio\n",
    ")\n",
    "from fairlearn.postprocessing import ThresholdOptimizer\n",
    "from fairlearn.reductions import (\n",
    "    ExponentiatedGradient, DemographicParity, EqualizedOdds\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Plotting setup\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (14, 8)\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"FAIRNESS MITIGATION\")\n",
    "print(\"=\" * 60)\n",
    "print(\"âœ“ Setup complete!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8bb5d63",
   "metadata": {},
   "source": [
    "## 1. Load Data, Models, and Baseline Results\n",
    "\n",
    "Load the preprocessed data, trained baseline models, and fairness analysis results from previous notebooks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b6ac14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine data directory (works for both local and Colab)\n",
    "if IN_COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    \n",
    "    possible_paths = [\n",
    "        Path('/content/drive/MyDrive/6.3950_project/processed_data'),\n",
    "        Path('/content/drive/Shareddrives/Insurance Fairness/processed_data'),\n",
    "        Path('/content/drive/MyDrive/insurance-fairness/results'),\n",
    "        Path('../results')\n",
    "    ]\n",
    "else:\n",
    "    possible_paths = [\n",
    "        Path('../results'),\n",
    "        Path('./results')\n",
    "    ]\n",
    "\n",
    "results_dir = None\n",
    "for path in possible_paths:\n",
    "    if path.exists() and (path / 'X_train.pkl').exists():\n",
    "        results_dir = path\n",
    "        break\n",
    "\n",
    "if results_dir is None:\n",
    "    error_msg = (\n",
    "        \"Could not find results directory. Please run notebooks 01, 02, and 03 first.\\n\"\n",
    "        f\"Tried paths: {', '.join([str(p) for p in possible_paths])}\"\n",
    "    )\n",
    "    raise FileNotFoundError(error_msg)\n",
    "\n",
    "print(f\"âœ“ Loading data from: {results_dir}\")\n",
    "\n",
    "# Load all data splits\n",
    "print(\"Loading data splits...\")\n",
    "with open(results_dir / 'X_train.pkl', 'rb') as f: X_train = pickle.load(f)\n",
    "with open(results_dir / 'X_val.pkl', 'rb') as f: X_val = pickle.load(f)\n",
    "with open(results_dir / 'X_test.pkl', 'rb') as f: X_test = pickle.load(f)\n",
    "with open(results_dir / 'y_train.pkl', 'rb') as f: y_train = pickle.load(f)\n",
    "with open(results_dir / 'y_val.pkl', 'rb') as f: y_val = pickle.load(f)\n",
    "with open(results_dir / 'y_test.pkl', 'rb') as f: y_test = pickle.load(f)\n",
    "with open(results_dir / 'protected_train.pkl', 'rb') as f: protected_train = pickle.load(f)\n",
    "with open(results_dir / 'protected_val.pkl', 'rb') as f: protected_val = pickle.load(f)\n",
    "with open(results_dir / 'protected_test.pkl', 'rb') as f: protected_test = pickle.load(f)\n",
    "with open(results_dir / 'feature_names.pkl', 'rb') as f: feature_names = pickle.load(f)\n",
    "\n",
    "# Load baseline models\n",
    "print(\"Loading baseline models...\")\n",
    "try:\n",
    "    with open(results_dir / 'logistic_regression_model.pkl', 'rb') as f:\n",
    "        lr_baseline = pickle.load(f)\n",
    "    print(\"  âœ“ Logistic Regression baseline loaded\")\n",
    "except FileNotFoundError:\n",
    "    raise FileNotFoundError(\"Logistic Regression baseline model not found. Please run notebook 02 first.\")\n",
    "except Exception as e:\n",
    "    raise RuntimeError(f\"Error loading Logistic Regression model: {type(e).__name__}: {e}\")\n",
    "\n",
    "try:\n",
    "    with open(results_dir / 'random_forest_model.pkl', 'rb') as f:\n",
    "        rf_baseline = pickle.load(f)\n",
    "    print(\"  âœ“ Random Forest baseline loaded\")\n",
    "except FileNotFoundError:\n",
    "    raise FileNotFoundError(\"Random Forest baseline model not found. Please run notebook 02 first.\")\n",
    "except Exception as e:\n",
    "    raise RuntimeError(f\"Error loading Random Forest model: {type(e).__name__}: {e}\")\n",
    "\n",
    "# Load baseline predictions\n",
    "print(\"Loading baseline predictions...\")\n",
    "try:\n",
    "    with open(results_dir / 'lr_test_predictions.pkl', 'rb') as f:\n",
    "        lr_test_pred_baseline = pickle.load(f)\n",
    "    with open(results_dir / 'lr_test_proba.pkl', 'rb') as f:\n",
    "        lr_test_proba_baseline = pickle.load(f)\n",
    "    print(\"  âœ“ Logistic Regression predictions loaded\")\n",
    "except FileNotFoundError:\n",
    "    raise FileNotFoundError(\"Logistic Regression predictions not found. Please run notebook 02 first.\")\n",
    "except Exception as e:\n",
    "    raise RuntimeError(f\"Error loading Logistic Regression predictions: {type(e).__name__}: {e}\")\n",
    "\n",
    "try:\n",
    "    with open(results_dir / 'rf_test_predictions.pkl', 'rb') as f:\n",
    "        rf_test_pred_baseline = pickle.load(f)\n",
    "    with open(results_dir / 'rf_test_proba.pkl', 'rb') as f:\n",
    "        rf_test_proba_baseline = pickle.load(f)\n",
    "    print(\"  âœ“ Random Forest predictions loaded\")\n",
    "except FileNotFoundError:\n",
    "    raise FileNotFoundError(\"Random Forest predictions not found. Please run notebook 02 first.\")\n",
    "except Exception as e:\n",
    "    raise RuntimeError(f\"Error loading Random Forest predictions: {type(e).__name__}: {e}\")\n",
    "\n",
    "# Load baseline fairness results\n",
    "print(\"Loading baseline fairness analysis...\")\n",
    "try:\n",
    "    with open(results_dir / 'baseline_fairness_analysis.json', 'r') as f:\n",
    "        baseline_fairness = json.load(f)\n",
    "    print(\"âœ“ Baseline fairness results loaded\")\n",
    "except FileNotFoundError:\n",
    "    print(\"âš  Warning: Baseline fairness analysis not found. Run notebook 03 first.\")\n",
    "    baseline_fairness = {}\n",
    "\n",
    "# Load metadata\n",
    "try:\n",
    "    with open(results_dir / 'preprocessing_metadata.json', 'r') as f:\n",
    "        metadata = json.load(f)\n",
    "    protected_attributes_list = metadata.get('protected_attributes', [])\n",
    "except (FileNotFoundError, json.JSONDecodeError, KeyError) as e:\n",
    "    # Fallback: infer from protected_test columns\n",
    "    print(f\"âš  Warning: Could not load metadata ({type(e).__name__}), inferring from protected_test\")\n",
    "    protected_attributes_list = protected_test.columns.tolist()\n",
    "\n",
    "# Validate data consistency\n",
    "print(\"\\nðŸ” Validating data consistency...\")\n",
    "assert len(y_train) == len(X_train), f\"Length mismatch: y_train ({len(y_train)}) vs X_train ({len(X_train)})\"\n",
    "assert len(y_val) == len(X_val), f\"Length mismatch: y_val ({len(y_val)}) vs X_val ({len(X_val)})\"\n",
    "assert len(y_test) == len(X_test), f\"Length mismatch: y_test ({len(y_test)}) vs X_test ({len(X_test)})\"\n",
    "\n",
    "# Validate protected attributes are DataFrames\n",
    "if not isinstance(protected_train, pd.DataFrame):\n",
    "    raise TypeError(f\"protected_train must be a pandas DataFrame, got {type(protected_train)}\")\n",
    "if not isinstance(protected_val, pd.DataFrame):\n",
    "    raise TypeError(f\"protected_val must be a pandas DataFrame, got {type(protected_val)}\")\n",
    "if not isinstance(protected_test, pd.DataFrame):\n",
    "    raise TypeError(f\"protected_test must be a pandas DataFrame, got {type(protected_test)}\")\n",
    "\n",
    "assert len(protected_train) == len(X_train), f\"Length mismatch: protected_train ({len(protected_train)}) vs X_train ({len(X_train)})\"\n",
    "assert len(protected_val) == len(X_val), f\"Length mismatch: protected_val ({len(protected_val)}) vs X_val ({len(X_val)})\"\n",
    "assert len(protected_test) == len(X_test), f\"Length mismatch: protected_test ({len(protected_test)}) vs X_test ({len(X_test)})\"\n",
    "assert len(lr_test_pred_baseline) == len(y_test), f\"Length mismatch: lr_test_pred_baseline ({len(lr_test_pred_baseline)}) vs y_test ({len(y_test)})\"\n",
    "assert len(rf_test_pred_baseline) == len(y_test), f\"Length mismatch: rf_test_pred_baseline ({len(rf_test_pred_baseline)}) vs y_test ({len(y_test)})\"\n",
    "assert len(lr_test_proba_baseline) == len(y_test), f\"Length mismatch: lr_test_proba_baseline ({len(lr_test_proba_baseline)}) vs y_test ({len(y_test)})\"\n",
    "assert len(rf_test_proba_baseline) == len(y_test), f\"Length mismatch: rf_test_proba_baseline ({len(rf_test_proba_baseline)}) vs y_test ({len(y_test)})\"\n",
    "\n",
    "# Validate models loaded successfully\n",
    "assert hasattr(lr_baseline, 'predict'), \"LR baseline model missing predict method\"\n",
    "assert hasattr(lr_baseline, 'predict_proba'), \"LR baseline model missing predict_proba method\"\n",
    "assert hasattr(rf_baseline, 'predict'), \"RF baseline model missing predict method\"\n",
    "assert hasattr(rf_baseline, 'predict_proba'), \"RF baseline model missing predict_proba method\"\n",
    "print(\"âœ“ All data validated successfully!\")\n",
    "\n",
    "print(f\"\\nâœ“ Data loaded!\")\n",
    "print(f\"   Training: {X_train.shape[0]:,} samples Ã— {X_train.shape[1]} features\")\n",
    "print(f\"   Validation: {X_val.shape[0]:,} samples\")\n",
    "print(f\"   Test: {X_test.shape[0]:,} samples\")\n",
    "print(f\"   Protected attributes: {', '.join(protected_attributes_list)}\")\n",
    "print(f\"   Class distribution (train): {(y_train == 1).mean():.2%} positive class\")\n",
    "\n",
    "# Define primary protected attribute once for use across all mitigation techniques\n",
    "primary_attr = protected_attributes_list[0] if protected_attributes_list else None\n",
    "if primary_attr:\n",
    "    print(f\"\\nðŸ“Œ Primary protected attribute for mitigation: {primary_attr}\")\n",
    "else:\n",
    "    print(\"\\nâš  Warning: No protected attributes found. Mitigation techniques may be limited.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "845f8858",
   "metadata": {},
   "source": [
    "## 2. Helper Functions for Evaluation\n",
    "\n",
    "Define functions to calculate and compare fairness metrics across different mitigation approaches.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40398c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_performance_metrics(y_true, y_pred, y_proba):\n",
    "    \"\"\"\n",
    "    Calculate comprehensive performance metrics.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    y_true : array-like\n",
    "        True labels\n",
    "    y_pred : array-like\n",
    "        Predicted labels\n",
    "    y_proba : array-like\n",
    "        Predicted probabilities for positive class\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    dict : Dictionary containing performance metrics\n",
    "    \"\"\"\n",
    "    # Input validation\n",
    "    y_true = np.asarray(y_true)\n",
    "    y_pred = np.asarray(y_pred)\n",
    "    y_proba = np.asarray(y_proba)\n",
    "    \n",
    "    if len(y_true) == 0 or len(y_pred) == 0 or len(y_proba) == 0:\n",
    "        raise ValueError(\"Input arrays cannot be empty\")\n",
    "    \n",
    "    if len(y_true) != len(y_pred) or len(y_true) != len(y_proba):\n",
    "        raise ValueError(\n",
    "            f\"Length mismatch: y_true ({len(y_true)}), y_pred ({len(y_pred)}), \"\n",
    "            f\"y_proba ({len(y_proba)})\"\n",
    "        )\n",
    "    \n",
    "    # Check for valid classification scenario\n",
    "    unique_labels = np.unique(y_true)\n",
    "    if len(unique_labels) == 0:\n",
    "        raise ValueError(\"y_true contains no labels\")\n",
    "    \n",
    "    # Calculate ROC-AUC only if we have both classes\n",
    "    has_both_classes = len(unique_labels) > 1\n",
    "    roc_auc = roc_auc_score(y_true, y_proba) if has_both_classes else 0.0\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy_score(y_true, y_pred),\n",
    "        'precision': precision_score(y_true, y_pred, zero_division=0),\n",
    "        'recall': recall_score(y_true, y_pred, zero_division=0),\n",
    "        'f1': f1_score(y_true, y_pred, zero_division=0),\n",
    "        'roc_auc': roc_auc\n",
    "    }\n",
    "\n",
    "\n",
    "def calculate_fairness_metrics(y_true, y_pred, sensitive_feature):\n",
    "    \"\"\"\n",
    "    Calculate fairness metrics for a given model and protected attribute.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    y_true : array-like\n",
    "        True labels\n",
    "    y_pred : array-like\n",
    "        Predicted labels\n",
    "    sensitive_feature : array-like\n",
    "        Protected attribute values\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    dict : Dictionary containing fairness metrics\n",
    "    \"\"\"\n",
    "    # Input validation\n",
    "    y_true = np.asarray(y_true)\n",
    "    y_pred = np.asarray(y_pred)\n",
    "    sensitive_feature = np.asarray(sensitive_feature)\n",
    "    \n",
    "    if len(y_true) == 0 or len(y_pred) == 0 or len(sensitive_feature) == 0:\n",
    "        raise ValueError(\"Input arrays cannot be empty\")\n",
    "    \n",
    "    if len(y_true) != len(y_pred) or len(y_true) != len(sensitive_feature):\n",
    "        raise ValueError(\n",
    "            f\"Length mismatch: y_true ({len(y_true)}), y_pred ({len(y_pred)}), \"\n",
    "            f\"sensitive_feature ({len(sensitive_feature)})\"\n",
    "        )\n",
    "    \n",
    "    # Check for valid groups\n",
    "    unique_groups = np.unique(sensitive_feature)\n",
    "    if len(unique_groups) < 2:\n",
    "        print(f\"âš  Warning: Only one group found in sensitive_feature. Fairness metrics may be undefined.\")\n",
    "    \n",
    "    # Calculate fairness metrics\n",
    "    dp_diff = demographic_parity_difference(y_true, y_pred, sensitive_features=sensitive_feature)\n",
    "    dp_ratio = demographic_parity_ratio(y_true, y_pred, sensitive_features=sensitive_feature)\n",
    "    eo_diff = equalized_odds_difference(y_true, y_pred, sensitive_features=sensitive_feature)\n",
    "    eo_ratio = equalized_odds_ratio(y_true, y_pred, sensitive_features=sensitive_feature)\n",
    "    \n",
    "    # Calculate precision parity with zero_division handling\n",
    "    precision_func = lambda y_t, y_p: precision_score(y_t, y_p, zero_division=0)\n",
    "    metric_frame = MetricFrame(\n",
    "        metrics={'precision': precision_func},\n",
    "        y_true=y_true,\n",
    "        y_pred=y_pred,\n",
    "        sensitive_features=sensitive_feature\n",
    "    )\n",
    "    precision_by_group = metric_frame.by_group['precision']\n",
    "    precision_diff = precision_by_group.max() - precision_by_group.min() if len(precision_by_group.dropna()) > 0 else np.nan\n",
    "    \n",
    "    return {\n",
    "        'demographic_parity_difference': dp_diff,\n",
    "        'demographic_parity_ratio': dp_ratio,\n",
    "        'equalized_odds_difference': eo_diff,\n",
    "        'equalized_odds_ratio': eo_ratio,\n",
    "        'precision_difference': precision_diff\n",
    "    }\n",
    "\n",
    "\n",
    "def evaluate_model(y_true, y_pred, y_proba, sensitive_feature, model_name, attr_name):\n",
    "    \"\"\"\n",
    "    Comprehensive evaluation combining performance and fairness metrics.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    y_true : array-like\n",
    "        True labels\n",
    "    y_pred : array-like\n",
    "        Predicted labels\n",
    "    y_proba : array-like\n",
    "        Predicted probabilities for positive class\n",
    "    sensitive_feature : array-like\n",
    "        Protected attribute values\n",
    "    model_name : str\n",
    "        Name of the model being evaluated\n",
    "    attr_name : str\n",
    "        Name of the protected attribute\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    dict : Dictionary containing combined performance and fairness metrics\n",
    "    \"\"\"\n",
    "    perf_metrics = calculate_performance_metrics(y_true, y_pred, y_proba)\n",
    "    fair_metrics = calculate_fairness_metrics(y_true, y_pred, sensitive_feature)\n",
    "    \n",
    "    return {\n",
    "        'model_name': model_name,\n",
    "        'attribute_name': attr_name,\n",
    "        **perf_metrics,\n",
    "        **fair_metrics\n",
    "    }\n",
    "\n",
    "\n",
    "print(\"âœ“ Helper functions defined\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6c10d44",
   "metadata": {},
   "source": [
    "## 3. Post-Processing: Threshold Optimization\n",
    "\n",
    "**Approach:** Adjust decision thresholds per protected group to achieve demographic parity while preserving model probabilities.\n",
    "\n",
    "**Rationale:** \n",
    "- Preserves model interpretability\n",
    "- No retraining required\n",
    "- Can be applied to any trained model\n",
    "- Trade-off: May reduce accuracy slightly\n",
    "\n",
    "**Configuration:**\n",
    "- Constraint: `demographic_parity` (can also use `equalized_odds`)\n",
    "- Preprocessing: Enabled to ensure optimal threshold selection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c7172a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store results for all mitigation techniques\n",
    "mitigation_results = {}\n",
    "mitigated_models = {}\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"POST-PROCESSING: THRESHOLD OPTIMIZATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# We'll apply threshold optimization to both baseline models\n",
    "# Using the primary protected attribute defined earlier\n",
    "\n",
    "if primary_attr is None:\n",
    "    print(\"âš  Warning: No protected attributes found. Skipping threshold optimization.\")\n",
    "else:\n",
    "    print(f\"\\nApplying threshold optimization for protected attribute: {primary_attr}\")\n",
    "    \n",
    "    # Prepare sensitive features for validation and test sets\n",
    "    sensitive_val = protected_val[primary_attr]\n",
    "    sensitive_test = protected_test[primary_attr]\n",
    "    \n",
    "    # Apply to Logistic Regression baseline\n",
    "    print(\"\\n--- Threshold Optimization for Logistic Regression ---\")\n",
    "    \n",
    "    # Create threshold optimizer\n",
    "    threshold_optimizer_lr = ThresholdOptimizer(\n",
    "        estimator=lr_baseline,\n",
    "        constraints=\"demographic_parity\",  # Can also use \"equalized_odds\"\n",
    "        preprocess=True\n",
    "    )\n",
    "    \n",
    "    print(\"Fitting threshold optimizer on validation set...\")\n",
    "    start_time = time.time()\n",
    "    threshold_optimizer_lr.fit(X_val, y_val, sensitive_features=sensitive_val)\n",
    "    fitting_time = time.time() - start_time\n",
    "    print(f\"âœ“ Threshold optimizer fitted in {fitting_time:.2f} seconds\")\n",
    "    \n",
    "    # Predict on test set\n",
    "    print(\"Generating predictions on test set...\")\n",
    "    lr_test_pred_threshold = threshold_optimizer_lr.predict(\n",
    "        X_test, sensitive_features=sensitive_test\n",
    "    )\n",
    "    lr_test_proba_threshold = lr_baseline.predict_proba(X_test)[:, 1]  # Use original probabilities\n",
    "    \n",
    "    # Evaluate\n",
    "    lr_threshold_metrics = {}\n",
    "    for attr in protected_attributes_list:\n",
    "        if attr in protected_test.columns:\n",
    "            metrics = evaluate_model(\n",
    "                y_test, lr_test_pred_threshold, lr_test_proba_threshold,\n",
    "                protected_test[attr], f\"LR_Threshold_{primary_attr}\", attr\n",
    "            )\n",
    "            lr_threshold_metrics[attr] = metrics\n",
    "    \n",
    "    mitigation_results['LR_ThresholdOptimizer'] = lr_threshold_metrics\n",
    "    mitigated_models['LR_ThresholdOptimizer'] = threshold_optimizer_lr\n",
    "    \n",
    "    print(f\"\\nâœ“ Threshold optimization complete for Logistic Regression\")\n",
    "    print(f\"   Accuracy: {lr_threshold_metrics.get(primary_attr, {}).get('accuracy', 0):.4f}\")\n",
    "    print(f\"   DP Difference: {lr_threshold_metrics.get(primary_attr, {}).get('demographic_parity_difference', 0):.4f}\")\n",
    "    \n",
    "    # Apply to Random Forest baseline\n",
    "    print(\"\\n--- Threshold Optimization for Random Forest ---\")\n",
    "    \n",
    "    # Create threshold optimizer\n",
    "    threshold_optimizer_rf = ThresholdOptimizer(\n",
    "        estimator=rf_baseline,\n",
    "        constraints=\"demographic_parity\",\n",
    "        preprocess=True\n",
    "    )\n",
    "    \n",
    "    print(\"Fitting threshold optimizer on validation set...\")\n",
    "    start_time = time.time()\n",
    "    threshold_optimizer_rf.fit(X_val, y_val, sensitive_features=sensitive_val)\n",
    "    fitting_time = time.time() - start_time\n",
    "    print(f\"âœ“ Threshold optimizer fitted in {fitting_time:.2f} seconds\")\n",
    "    \n",
    "    # Predict on test set\n",
    "    print(\"Generating predictions on test set...\")\n",
    "    rf_test_pred_threshold = threshold_optimizer_rf.predict(\n",
    "        X_test, sensitive_features=sensitive_test\n",
    "    )\n",
    "    rf_test_proba_threshold = rf_baseline.predict_proba(X_test)[:, 1]  # Use original probabilities\n",
    "    \n",
    "    # Evaluate\n",
    "    rf_threshold_metrics = {}\n",
    "    for attr in protected_attributes_list:\n",
    "        if attr in protected_test.columns:\n",
    "            metrics = evaluate_model(\n",
    "                y_test, rf_test_pred_threshold, rf_test_proba_threshold,\n",
    "                protected_test[attr], f\"RF_Threshold_{primary_attr}\", attr\n",
    "            )\n",
    "            rf_threshold_metrics[attr] = metrics\n",
    "    \n",
    "    mitigation_results['RF_ThresholdOptimizer'] = rf_threshold_metrics\n",
    "    mitigated_models['RF_ThresholdOptimizer'] = threshold_optimizer_rf\n",
    "    \n",
    "    print(f\"\\nâœ“ Threshold optimization complete for Random Forest\")\n",
    "    print(f\"   Accuracy: {rf_threshold_metrics.get(primary_attr, {}).get('accuracy', 0):.4f}\")\n",
    "    print(f\"   DP Difference: {rf_threshold_metrics.get(primary_attr, {}).get('demographic_parity_difference', 0):.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cecc0302",
   "metadata": {},
   "source": [
    "## 4. In-Processing: ExponentiatedGradient\n",
    "\n",
    "**Approach:** Train a fair model using constraint-based optimization that directly enforces fairness constraints during training.\n",
    "\n",
    "**Rationale:**\n",
    "- Enforces fairness constraints directly in the learning process\n",
    "- More principled approach than post-processing\n",
    "- Can optimize for multiple fairness constraints\n",
    "- Trade-off: More computationally expensive, requires retraining\n",
    "\n",
    "**Configuration:**\n",
    "- Base estimator: Logistic Regression (faster than Random Forest for this algorithm)\n",
    "- Constraint types: Demographic Parity and Equalized Odds\n",
    "- Epsilon (relaxation parameter): 0.01 (smaller = stricter constraint, typically ranges 0.001-0.1)\n",
    "- Solver: 'lbfgs' (Limited-memory BFGS, suitable for small-medium datasets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e1e8d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"IN-PROCESSING: EXPONENTIATED GRADIENT\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# ExponentiatedGradient can be slow, so we'll apply it to one model and one constraint type\n",
    "# Using Logistic Regression as the base estimator (faster than Random Forest)\n",
    "# Using the primary protected attribute defined earlier\n",
    "\n",
    "if primary_attr is None:\n",
    "    print(\"âš  Warning: No protected attributes found. Skipping ExponentiatedGradient.\")\n",
    "else:\n",
    "    print(f\"\\nTraining ExponentiatedGradient model for protected attribute: {primary_attr}\")\n",
    "    \n",
    "    # Prepare sensitive features\n",
    "    sensitive_train = protected_train[primary_attr]\n",
    "    sensitive_test = protected_test[primary_attr]\n",
    "    \n",
    "    # Base estimator\n",
    "    base_estimator = LogisticRegression(\n",
    "        class_weight='balanced',\n",
    "        max_iter=1000,\n",
    "        random_state=42,\n",
    "        solver='lbfgs'\n",
    "    )\n",
    "    \n",
    "    # Hyperparameters for ExponentiatedGradient\n",
    "    EPSILON = 0.01  # Relaxation parameter: smaller = stricter fairness constraint (typical range: 0.001-0.1)\n",
    "    \n",
    "    # Create ExponentiatedGradient with demographic parity constraint\n",
    "    print(\"\\n--- ExponentiatedGradient with Demographic Parity Constraint ---\")\n",
    "    print(f\"Training with epsilon={EPSILON} (this may take several minutes)...\")\n",
    "    \n",
    "    exp_grad_dp = ExponentiatedGradient(\n",
    "        estimator=base_estimator,\n",
    "        constraints=DemographicParity(),\n",
    "        eps=EPSILON\n",
    "    )\n",
    "    \n",
    "    start_time = time.time()\n",
    "    exp_grad_dp.fit(X_train, y_train, sensitive_features=sensitive_train)\n",
    "    training_time = time.time() - start_time\n",
    "    print(f\"âœ“ ExponentiatedGradient training completed in {training_time:.2f} seconds\")\n",
    "    \n",
    "    # Predict on test set\n",
    "    print(\"Generating predictions on test set...\")\n",
    "    exp_grad_dp_test_pred = exp_grad_dp.predict(X_test)\n",
    "    exp_grad_dp_test_proba = exp_grad_dp.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    # Evaluate\n",
    "    exp_grad_dp_metrics = {}\n",
    "    for attr in protected_attributes_list:\n",
    "        if attr in protected_test.columns:\n",
    "            metrics = evaluate_model(\n",
    "                y_test, exp_grad_dp_test_pred, exp_grad_dp_test_proba,\n",
    "                protected_test[attr], \"ExpGrad_DP\", attr\n",
    "            )\n",
    "            exp_grad_dp_metrics[attr] = metrics\n",
    "    \n",
    "    mitigation_results['ExpGrad_DemographicParity'] = exp_grad_dp_metrics\n",
    "    mitigated_models['ExpGrad_DemographicParity'] = exp_grad_dp\n",
    "    \n",
    "    print(f\"\\nâœ“ ExponentiatedGradient (DP) training complete\")\n",
    "    print(f\"   Accuracy: {exp_grad_dp_metrics.get(primary_attr, {}).get('accuracy', 0):.4f}\")\n",
    "    print(f\"   DP Difference: {exp_grad_dp_metrics.get(primary_attr, {}).get('demographic_parity_difference', 0):.4f}\")\n",
    "    \n",
    "    # Optionally: Train with Equalized Odds constraint\n",
    "    print(\"\\n--- ExponentiatedGradient with Equalized Odds Constraint ---\")\n",
    "    print(\"Training (this may take several minutes)...\")\n",
    "    \n",
    "    exp_grad_eo = ExponentiatedGradient(\n",
    "        estimator=LogisticRegression(class_weight='balanced', max_iter=1000, random_state=42, solver='lbfgs'),\n",
    "        constraints=EqualizedOdds(),\n",
    "        eps=EPSILON  # Using the same epsilon as defined above\n",
    "    )\n",
    "    \n",
    "    start_time = time.time()\n",
    "    exp_grad_eo.fit(X_train, y_train, sensitive_features=sensitive_train)\n",
    "    training_time = time.time() - start_time\n",
    "    print(f\"âœ“ ExponentiatedGradient (EO) training completed in {training_time:.2f} seconds\")\n",
    "    \n",
    "    # Predict on test set\n",
    "    print(\"Generating predictions on test set...\")\n",
    "    exp_grad_eo_test_pred = exp_grad_eo.predict(X_test)\n",
    "    exp_grad_eo_test_proba = exp_grad_eo.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    # Evaluate\n",
    "    exp_grad_eo_metrics = {}\n",
    "    for attr in protected_attributes_list:\n",
    "        if attr in protected_test.columns:\n",
    "            metrics = evaluate_model(\n",
    "                y_test, exp_grad_eo_test_pred, exp_grad_eo_test_proba,\n",
    "                protected_test[attr], \"ExpGrad_EO\", attr\n",
    "            )\n",
    "            exp_grad_eo_metrics[attr] = metrics\n",
    "    \n",
    "    mitigation_results['ExpGrad_EqualizedOdds'] = exp_grad_eo_metrics\n",
    "    mitigated_models['ExpGrad_EqualizedOdds'] = exp_grad_eo\n",
    "    \n",
    "    print(f\"\\nâœ“ ExponentiatedGradient (EO) training complete\")\n",
    "    print(f\"   Accuracy: {exp_grad_eo_metrics.get(primary_attr, {}).get('accuracy', 0):.4f}\")\n",
    "    print(f\"   EO Difference: {exp_grad_eo_metrics.get(primary_attr, {}).get('equalized_odds_difference', 0):.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4fa6881",
   "metadata": {},
   "source": [
    "## 5. Preprocessing: Sample Reweighting\n",
    "\n",
    "**Approach:** Adjust sample weights during training to balance groups and reduce bias in the learned model.\n",
    "\n",
    "**Rationale:**\n",
    "- Simple and interpretable\n",
    "- Can be combined with any classifier\n",
    "- Preserves model structure\n",
    "- Trade-off: May require tuning of weighting scheme\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba877be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"PREPROCESSING: SAMPLE REWEIGHTING\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "def calculate_reweights(y, sensitive_feature):\n",
    "    \"\"\"\n",
    "    Calculate sample weights to balance groups.\n",
    "    \n",
    "    Strategy: Weight samples inversely proportional to their group-target combination frequency.\n",
    "    This gives more weight to underrepresented group-target combinations.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    y : array-like\n",
    "        Target labels\n",
    "    sensitive_feature : array-like\n",
    "        Protected attribute values\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    weights : np.ndarray\n",
    "        Sample weights normalized to average 1.0\n",
    "    \"\"\"\n",
    "    # Input validation\n",
    "    y = np.asarray(y)\n",
    "    sensitive_feature = np.asarray(sensitive_feature)\n",
    "    \n",
    "    if len(y) == 0 or len(sensitive_feature) == 0:\n",
    "        raise ValueError(\"Input arrays cannot be empty\")\n",
    "    \n",
    "    if len(y) != len(sensitive_feature):\n",
    "        raise ValueError(f\"Length mismatch: y ({len(y)}) vs sensitive_feature ({len(sensitive_feature)})\")\n",
    "    \n",
    "    # Check for valid groups\n",
    "    unique_groups = np.unique(sensitive_feature)\n",
    "    unique_targets = np.unique(y)\n",
    "    \n",
    "    if len(unique_groups) < 2:\n",
    "        print(f\"âš  Warning: Only one group found in sensitive_feature, using uniform weights\")\n",
    "        return np.ones(len(y))\n",
    "    \n",
    "    if len(unique_targets) < 2:\n",
    "        print(f\"âš  Warning: Only one class found in y, using uniform weights\")\n",
    "        return np.ones(len(y))\n",
    "    \n",
    "    weights = np.ones(len(y))\n",
    "    \n",
    "    # Calculate group-target combination frequencies\n",
    "    for group in unique_groups:\n",
    "        for target in unique_targets:\n",
    "            mask = (sensitive_feature == group) & (y == target)\n",
    "            count = mask.sum()\n",
    "            \n",
    "            if count > 0:\n",
    "                # Weight inversely proportional to frequency\n",
    "                weights[mask] = len(y) / (count * len(unique_groups) * len(unique_targets))\n",
    "    \n",
    "    # Normalize weights so they average to 1\n",
    "    if weights.mean() > 0:\n",
    "        weights = weights / weights.mean()\n",
    "    else:\n",
    "        print(\"âš  Warning: All weights are zero, using uniform weights\")\n",
    "        weights = np.ones(len(y))\n",
    "    \n",
    "    return weights\n",
    "\n",
    "# Using the primary protected attribute defined earlier\n",
    "\n",
    "if primary_attr is None:\n",
    "    print(\"âš  Warning: No protected attributes found. Skipping reweighting.\")\n",
    "else:\n",
    "    print(f\"\\nApplying sample reweighting for protected attribute: {primary_attr}\")\n",
    "    \n",
    "    # Calculate weights for training set\n",
    "    print(\"Calculating sample weights...\")\n",
    "    sample_weights = calculate_reweights(y_train, protected_train[primary_attr])\n",
    "    \n",
    "    # Train Logistic Regression with reweighting\n",
    "    print(\"\\n--- Reweighted Logistic Regression ---\")\n",
    "    print(\"Training...\")\n",
    "    \n",
    "    lr_reweighted = LogisticRegression(\n",
    "        class_weight='balanced',  # Also use class balancing\n",
    "        max_iter=1000,\n",
    "        random_state=42,\n",
    "        solver='lbfgs'\n",
    "    )\n",
    "    \n",
    "    start_time = time.time()\n",
    "    lr_reweighted.fit(X_train, y_train, sample_weight=sample_weights)\n",
    "    training_time = time.time() - start_time\n",
    "    print(f\"âœ“ Reweighted Logistic Regression trained in {training_time:.2f} seconds\")\n",
    "    \n",
    "    # Predict on test set\n",
    "    print(\"Generating predictions on test set...\")\n",
    "    lr_reweighted_test_pred = lr_reweighted.predict(X_test)\n",
    "    lr_reweighted_test_proba = lr_reweighted.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    # Evaluate\n",
    "    lr_reweighted_metrics = {}\n",
    "    for attr in protected_attributes_list:\n",
    "        if attr in protected_test.columns:\n",
    "            metrics = evaluate_model(\n",
    "                y_test, lr_reweighted_test_pred, lr_reweighted_test_proba,\n",
    "                protected_test[attr], \"LR_Reweighted\", attr\n",
    "            )\n",
    "            lr_reweighted_metrics[attr] = metrics\n",
    "    \n",
    "    mitigation_results['LR_Reweighted'] = lr_reweighted_metrics\n",
    "    mitigated_models['LR_Reweighted'] = lr_reweighted\n",
    "    \n",
    "    print(f\"\\nâœ“ Reweighted Logistic Regression training complete\")\n",
    "    print(f\"   Accuracy: {lr_reweighted_metrics.get(primary_attr, {}).get('accuracy', 0):.4f}\")\n",
    "    print(f\"   DP Difference: {lr_reweighted_metrics.get(primary_attr, {}).get('demographic_parity_difference', 0):.4f}\")\n",
    "    \n",
    "    # Train Random Forest with reweighting\n",
    "    print(\"\\n--- Reweighted Random Forest ---\")\n",
    "    print(\"Training (this may take a minute)...\")\n",
    "    \n",
    "    rf_reweighted = RandomForestClassifier(\n",
    "        n_estimators=100,\n",
    "        max_depth=10,\n",
    "        class_weight='balanced',\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    start_time = time.time()\n",
    "    rf_reweighted.fit(X_train, y_train, sample_weight=sample_weights)\n",
    "    training_time = time.time() - start_time\n",
    "    print(f\"âœ“ Reweighted Random Forest trained in {training_time:.2f} seconds\")\n",
    "    \n",
    "    # Predict on test set\n",
    "    print(\"Generating predictions on test set...\")\n",
    "    rf_reweighted_test_pred = rf_reweighted.predict(X_test)\n",
    "    rf_reweighted_test_proba = rf_reweighted.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    # Evaluate\n",
    "    rf_reweighted_metrics = {}\n",
    "    for attr in protected_attributes_list:\n",
    "        if attr in protected_test.columns:\n",
    "            metrics = evaluate_model(\n",
    "                y_test, rf_reweighted_test_pred, rf_reweighted_test_proba,\n",
    "                protected_test[attr], \"RF_Reweighted\", attr\n",
    "            )\n",
    "            rf_reweighted_metrics[attr] = metrics\n",
    "    \n",
    "    mitigation_results['RF_Reweighted'] = rf_reweighted_metrics\n",
    "    mitigated_models['RF_Reweighted'] = rf_reweighted\n",
    "    \n",
    "    print(f\"\\nâœ“ Reweighted Random Forest training complete\")\n",
    "    print(f\"   Accuracy: {rf_reweighted_metrics.get(primary_attr, {}).get('accuracy', 0):.4f}\")\n",
    "    print(f\"   DP Difference: {rf_reweighted_metrics.get(primary_attr, {}).get('demographic_parity_difference', 0):.4f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72edd568",
   "metadata": {},
   "source": [
    "## 6. Compare All Approaches\n",
    "\n",
    "Create a comprehensive comparison of baseline models and all mitigation techniques, evaluating both performance and fairness metrics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a61b2ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"COMPREHENSIVE COMPARISON\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# First, evaluate baseline models for comparison\n",
    "print(\"\\nEvaluating baseline models for comparison...\")\n",
    "\n",
    "baseline_results = {}\n",
    "\n",
    "# Logistic Regression Baseline\n",
    "print(\"  - Logistic Regression baseline...\")\n",
    "lr_baseline_metrics = {}\n",
    "for attr in protected_attributes_list:\n",
    "    if attr in protected_test.columns:\n",
    "        metrics = evaluate_model(\n",
    "            y_test, lr_test_pred_baseline, lr_test_proba_baseline,\n",
    "            protected_test[attr], \"LR_Baseline\", attr\n",
    "        )\n",
    "        lr_baseline_metrics[attr] = metrics\n",
    "baseline_results['LR_Baseline'] = lr_baseline_metrics\n",
    "\n",
    "# Random Forest Baseline\n",
    "print(\"  - Random Forest baseline...\")\n",
    "rf_baseline_metrics = {}\n",
    "for attr in protected_attributes_list:\n",
    "    if attr in protected_test.columns:\n",
    "        metrics = evaluate_model(\n",
    "            y_test, rf_test_pred_baseline, rf_test_proba_baseline,\n",
    "            protected_test[attr], \"RF_Baseline\", attr\n",
    "        )\n",
    "        rf_baseline_metrics[attr] = metrics\n",
    "baseline_results['RF_Baseline'] = rf_baseline_metrics\n",
    "\n",
    "# Combine all results\n",
    "all_results = {**baseline_results, **mitigation_results}\n",
    "\n",
    "# Create comprehensive comparison table\n",
    "print(\"\\nCreating comparison table...\")\n",
    "# Using the primary protected attribute defined earlier\n",
    "\n",
    "if primary_attr:\n",
    "    comparison_rows = []\n",
    "    \n",
    "    for approach_name, attr_results in all_results.items():\n",
    "        if primary_attr in attr_results:\n",
    "            metrics = attr_results[primary_attr]\n",
    "            comparison_rows.append({\n",
    "                'Approach': approach_name,\n",
    "                'Accuracy': metrics.get('accuracy', np.nan),\n",
    "                'Precision': metrics.get('precision', np.nan),\n",
    "                'Recall': metrics.get('recall', np.nan),\n",
    "                'F1-Score': metrics.get('f1', np.nan),\n",
    "                'ROC-AUC': metrics.get('roc_auc', np.nan),\n",
    "                'DP Difference': metrics.get('demographic_parity_difference', np.nan),\n",
    "                'EO Difference': metrics.get('equalized_odds_difference', np.nan),\n",
    "                'Precision Diff': metrics.get('precision_difference', np.nan)\n",
    "            })\n",
    "    \n",
    "    comparison_df = pd.DataFrame(comparison_rows)\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 100)\n",
    "    print(f\"COMPREHENSIVE COMPARISON - Protected Attribute: {primary_attr}\")\n",
    "    print(\"=\" * 100)\n",
    "    print(\"\\nAll metrics calculated on test set.\")\n",
    "    print(comparison_df.round(4).to_string(index=False))\n",
    "    \n",
    "    # Display key findings (with NaN handling)\n",
    "    print(\"\\nðŸ“Š Key Findings:\")\n",
    "    if not comparison_df['Accuracy'].isna().all():\n",
    "        best_acc_idx = comparison_df['Accuracy'].idxmax()\n",
    "        best_acc_val = comparison_df.loc[best_acc_idx, 'Accuracy']\n",
    "        best_acc_name = comparison_df.loc[best_acc_idx, 'Approach']\n",
    "        print(f\"   Best Accuracy: {best_acc_val:.4f} ({best_acc_name})\")\n",
    "    else:\n",
    "        print(\"   Best Accuracy: No valid data\")\n",
    "    \n",
    "    if not comparison_df['DP Difference'].isna().all():\n",
    "        best_dp_idx = comparison_df['DP Difference'].idxmin()\n",
    "        best_dp_val = comparison_df.loc[best_dp_idx, 'DP Difference']\n",
    "        best_dp_name = comparison_df.loc[best_dp_idx, 'Approach']\n",
    "        print(f\"   Best DP Difference (lowest): {best_dp_val:.4f} ({best_dp_name})\")\n",
    "    else:\n",
    "        print(\"   Best DP Difference: No valid data\")\n",
    "    \n",
    "    if not comparison_df['EO Difference'].isna().all():\n",
    "        best_eo_idx = comparison_df['EO Difference'].idxmin()\n",
    "        best_eo_val = comparison_df.loc[best_eo_idx, 'EO Difference']\n",
    "        best_eo_name = comparison_df.loc[best_eo_idx, 'Approach']\n",
    "        print(f\"   Best EO Difference (lowest): {best_eo_val:.4f} ({best_eo_name})\")\n",
    "    else:\n",
    "        print(\"   Best EO Difference: No valid data\")\n",
    "    \n",
    "    # Calculate accuracy vs fairness tradeoffs\n",
    "    print(\"\\nðŸ“ˆ Accuracy-Fairness Tradeoffs:\")\n",
    "    \n",
    "    # Check if baseline is available for comparison\n",
    "    baseline_available = False\n",
    "    baseline_acc = None\n",
    "    baseline_dp = None\n",
    "    \n",
    "    if 'LR_Baseline' in baseline_results and primary_attr in baseline_results['LR_Baseline']:\n",
    "        baseline_metrics = baseline_results['LR_Baseline'][primary_attr]\n",
    "        baseline_acc = baseline_metrics.get('accuracy', None)\n",
    "        baseline_dp = baseline_metrics.get('demographic_parity_difference', None)\n",
    "        baseline_available = baseline_acc is not None and baseline_dp is not None\n",
    "    \n",
    "    if baseline_available:\n",
    "        for _, row in comparison_df.iterrows():\n",
    "            print(f\"   {row['Approach']}:\")\n",
    "            \n",
    "            # Handle NaN values in metrics\n",
    "            if not pd.isna(row['Accuracy']):\n",
    "                accuracy_loss = baseline_acc - row['Accuracy']\n",
    "                print(f\"      Accuracy change: {accuracy_loss:+.4f}\")\n",
    "            else:\n",
    "                print(f\"      Accuracy change: N/A (NaN)\")\n",
    "            \n",
    "            if not pd.isna(row['DP Difference']):\n",
    "                dp_improvement = baseline_dp - row['DP Difference']\n",
    "                print(f\"      DP Difference improvement: {dp_improvement:+.4f}\")\n",
    "            else:\n",
    "                print(f\"      DP Difference improvement: N/A (NaN)\")\n",
    "    else:\n",
    "        print(\"   âš  Baseline metrics not available for comparison\")\n",
    "else:\n",
    "    print(\"âš  Warning: No primary attribute found. Cannot create comparison table.\")\n",
    "    comparison_df = pd.DataFrame()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d3aa77a",
   "metadata": {},
   "source": [
    "## 7. Fairness-Accuracy Tradeoff Visualization\n",
    "\n",
    "Visualize the tradeoff between accuracy and fairness for all approaches. This helps identify which mitigation techniques achieve the best balance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ecf6c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if primary_attr and len(comparison_df) > 0:\n",
    "    print(\"=\" * 60)\n",
    "    print(\"FAIRNESS-ACCURACY TRADEOFF VISUALIZATION\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Filter out rows with NaN values for visualization\n",
    "    comparison_df_clean = comparison_df.dropna(subset=['DP Difference', 'Accuracy', 'EO Difference'])\n",
    "    \n",
    "    if len(comparison_df_clean) == 0:\n",
    "        print(\"âš  Warning: No valid data points for visualization (all NaN).\")\n",
    "    else:\n",
    "        # Create tradeoff plots\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "        \n",
    "        # Plot 1: Accuracy vs Demographic Parity Difference\n",
    "        ax1 = axes[0]\n",
    "        \n",
    "        # Filter valid points for DP plot\n",
    "        valid_dp = comparison_df_clean.dropna(subset=['DP Difference', 'Accuracy'])\n",
    "        if len(valid_dp) > 0:\n",
    "            scatter1 = ax1.scatter(\n",
    "                valid_dp['DP Difference'],\n",
    "                valid_dp['Accuracy'],\n",
    "                s=150,\n",
    "                alpha=0.7,\n",
    "                c=range(len(valid_dp)),\n",
    "                cmap='viridis',\n",
    "                edgecolors='black',\n",
    "                linewidths=1.5\n",
    "            )\n",
    "            \n",
    "            # Annotate points\n",
    "            for idx, row in valid_dp.iterrows():\n",
    "                if not (pd.isna(row['DP Difference']) or pd.isna(row['Accuracy'])):\n",
    "                    ax1.annotate(\n",
    "                        row['Approach'],\n",
    "                        (row['DP Difference'], row['Accuracy']),\n",
    "                        fontsize=9,\n",
    "                        alpha=0.8,\n",
    "                        xytext=(5, 5),\n",
    "                        textcoords='offset points'\n",
    "                    )\n",
    "        else:\n",
    "            print(\"âš  Warning: No valid data points for DP Difference plot\")\n",
    "        \n",
    "        # Set up axes labels even if no data\n",
    "        ax1.set_xlabel('Demographic Parity Difference (lower is better)', fontsize=12, fontweight='bold')\n",
    "        ax1.set_ylabel('Accuracy', fontsize=12, fontweight='bold')\n",
    "        ax1.set_title('Accuracy vs Demographic Parity Tradeoff', fontsize=14, fontweight='bold')\n",
    "        ax1.grid(alpha=0.3)\n",
    "        \n",
    "        # Add reference lines for baseline (if available and valid)\n",
    "        baseline_mask = comparison_df['Approach'] == 'LR_Baseline'\n",
    "        if baseline_mask.any():\n",
    "            baseline_row = comparison_df[baseline_mask].iloc[0]\n",
    "            if not (pd.isna(baseline_row['Accuracy']) or pd.isna(baseline_row['DP Difference'])):\n",
    "                ax1.axhline(y=baseline_row['Accuracy'], color='red', linestyle='--', alpha=0.5, linewidth=2, label='Baseline Accuracy')\n",
    "                ax1.axvline(x=baseline_row['DP Difference'], color='red', linestyle='--', alpha=0.5, linewidth=2, label='Baseline DP Diff')\n",
    "                ax1.scatter([baseline_row['DP Difference']], [baseline_row['Accuracy']], \n",
    "                           s=200, color='red', marker='*', edgecolors='black', linewidths=2, \n",
    "                           zorder=5, label='Baseline')\n",
    "        \n",
    "        ax1.legend(loc='best')\n",
    "        \n",
    "        # Plot 2: Accuracy vs Equalized Odds Difference\n",
    "        ax2 = axes[1]\n",
    "        valid_eo = comparison_df_clean.dropna(subset=['EO Difference', 'Accuracy'])\n",
    "        if len(valid_eo) > 0:\n",
    "            scatter2 = ax2.scatter(\n",
    "                valid_eo['EO Difference'],\n",
    "                valid_eo['Accuracy'],\n",
    "                s=150,\n",
    "                alpha=0.7,\n",
    "                c=range(len(valid_eo)),\n",
    "                cmap='plasma',\n",
    "                edgecolors='black',\n",
    "                linewidths=1.5\n",
    "            )\n",
    "            \n",
    "            # Annotate points\n",
    "            for idx, row in valid_eo.iterrows():\n",
    "                if not (pd.isna(row['EO Difference']) or pd.isna(row['Accuracy'])):\n",
    "                    ax2.annotate(\n",
    "                        row['Approach'],\n",
    "                        (row['EO Difference'], row['Accuracy']),\n",
    "                        fontsize=9,\n",
    "                        alpha=0.8,\n",
    "                        xytext=(5, 5),\n",
    "                        textcoords='offset points'\n",
    "                    )\n",
    "        else:\n",
    "            print(\"âš  Warning: No valid data points for EO Difference plot\")\n",
    "        \n",
    "        ax2.set_xlabel('Equalized Odds Difference (lower is better)', fontsize=12, fontweight='bold')\n",
    "        ax2.set_ylabel('Accuracy', fontsize=12, fontweight='bold')\n",
    "        ax2.set_title('Accuracy vs Equalized Odds Tradeoff', fontsize=14, fontweight='bold')\n",
    "        ax2.grid(alpha=0.3)\n",
    "        \n",
    "        # Add reference lines for baseline (if available and valid)\n",
    "        if baseline_mask.any():\n",
    "            baseline_row = comparison_df[baseline_mask].iloc[0]\n",
    "            if not (pd.isna(baseline_row['Accuracy']) or pd.isna(baseline_row['EO Difference'])):\n",
    "                ax2.axhline(y=baseline_row['Accuracy'], color='red', linestyle='--', alpha=0.5, linewidth=2, label='Baseline Accuracy')\n",
    "                ax2.axvline(x=baseline_row['EO Difference'], color='red', linestyle='--', alpha=0.5, linewidth=2, label='Baseline EO Diff')\n",
    "                ax2.scatter([baseline_row['EO Difference']], [baseline_row['Accuracy']], \n",
    "                           s=200, color='red', marker='*', edgecolors='black', linewidths=2, \n",
    "                           zorder=5, label='Baseline')\n",
    "        \n",
    "        ax2.legend(loc='best')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Create bar chart comparison\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "        fig.suptitle(f'Comprehensive Model Comparison - {primary_attr}', fontsize=16, fontweight='bold', y=1.00)\n",
    "        \n",
    "        # Accuracy comparison\n",
    "        ax = axes[0, 0]\n",
    "        comparison_df_sorted = comparison_df.dropna(subset=['Accuracy']).sort_values('Accuracy', ascending=False)\n",
    "        if len(comparison_df_sorted) > 0:\n",
    "            colors = ['red' if 'Baseline' in name else 'steelblue' for name in comparison_df_sorted['Approach']]\n",
    "            ax.barh(range(len(comparison_df_sorted)), comparison_df_sorted['Accuracy'], color=colors, alpha=0.8, edgecolor='black')\n",
    "            ax.set_yticks(range(len(comparison_df_sorted)))\n",
    "            ax.set_yticklabels(comparison_df_sorted['Approach'], fontsize=9)\n",
    "            ax.set_xlabel('Accuracy', fontsize=12, fontweight='bold')\n",
    "            ax.set_title('Accuracy Comparison', fontsize=12, fontweight='bold')\n",
    "            ax.grid(axis='x', alpha=0.3)\n",
    "            for i, v in enumerate(comparison_df_sorted['Accuracy']):\n",
    "                if not pd.isna(v):\n",
    "                    ax.text(v, i, f'{v:.4f}', va='center', fontweight='bold', fontsize=9)\n",
    "            ax.invert_yaxis()\n",
    "        else:\n",
    "            ax.text(0.5, 0.5, 'No valid data', ha='center', va='center', transform=ax.transAxes)\n",
    "            ax.set_title('Accuracy Comparison', fontsize=12, fontweight='bold')\n",
    "        \n",
    "        # DP Difference comparison\n",
    "        ax = axes[0, 1]\n",
    "        comparison_df_sorted_dp = comparison_df.dropna(subset=['DP Difference']).sort_values('DP Difference', ascending=True)\n",
    "        if len(comparison_df_sorted_dp) > 0:\n",
    "            colors = ['red' if 'Baseline' in name else 'coral' for name in comparison_df_sorted_dp['Approach']]\n",
    "            ax.barh(range(len(comparison_df_sorted_dp)), comparison_df_sorted_dp['DP Difference'], color=colors, alpha=0.8, edgecolor='black')\n",
    "            ax.set_yticks(range(len(comparison_df_sorted_dp)))\n",
    "            ax.set_yticklabels(comparison_df_sorted_dp['Approach'], fontsize=9)\n",
    "            ax.set_xlabel('Demographic Parity Difference', fontsize=12, fontweight='bold')\n",
    "            ax.set_title('Fairness Comparison (DP Difference - Lower is Better)', fontsize=12, fontweight='bold')\n",
    "            ax.grid(axis='x', alpha=0.3)\n",
    "            for i, v in enumerate(comparison_df_sorted_dp['DP Difference']):\n",
    "                if not pd.isna(v):\n",
    "                    ax.text(v, i, f'{v:.4f}', va='center', fontweight='bold', fontsize=9)\n",
    "            ax.invert_yaxis()\n",
    "        else:\n",
    "            ax.text(0.5, 0.5, 'No valid data', ha='center', va='center', transform=ax.transAxes)\n",
    "            ax.set_title('Fairness Comparison (DP Difference - Lower is Better)', fontsize=12, fontweight='bold')\n",
    "        \n",
    "        # EO Difference comparison\n",
    "        ax = axes[1, 0]\n",
    "        comparison_df_sorted_eo = comparison_df.dropna(subset=['EO Difference']).sort_values('EO Difference', ascending=True)\n",
    "        if len(comparison_df_sorted_eo) > 0:\n",
    "            colors = ['red' if 'Baseline' in name else 'purple' for name in comparison_df_sorted_eo['Approach']]\n",
    "            ax.barh(range(len(comparison_df_sorted_eo)), comparison_df_sorted_eo['EO Difference'], color=colors, alpha=0.8, edgecolor='black')\n",
    "            ax.set_yticks(range(len(comparison_df_sorted_eo)))\n",
    "            ax.set_yticklabels(comparison_df_sorted_eo['Approach'], fontsize=9)\n",
    "            ax.set_xlabel('Equalized Odds Difference', fontsize=12, fontweight='bold')\n",
    "            ax.set_title('Fairness Comparison (EO Difference - Lower is Better)', fontsize=12, fontweight='bold')\n",
    "            ax.grid(axis='x', alpha=0.3)\n",
    "            for i, v in enumerate(comparison_df_sorted_eo['EO Difference']):\n",
    "                if not pd.isna(v):\n",
    "                    ax.text(v, i, f'{v:.4f}', va='center', fontweight='bold', fontsize=9)\n",
    "            ax.invert_yaxis()\n",
    "        else:\n",
    "            ax.text(0.5, 0.5, 'No valid data', ha='center', va='center', transform=ax.transAxes)\n",
    "            ax.set_title('Fairness Comparison (EO Difference - Lower is Better)', fontsize=12, fontweight='bold')\n",
    "        \n",
    "        # F1-Score comparison\n",
    "        ax = axes[1, 1]\n",
    "        comparison_df_sorted_f1 = comparison_df.dropna(subset=['F1-Score']).sort_values('F1-Score', ascending=False)\n",
    "        if len(comparison_df_sorted_f1) > 0:\n",
    "            colors = ['red' if 'Baseline' in name else 'forestgreen' for name in comparison_df_sorted_f1['Approach']]\n",
    "            ax.barh(range(len(comparison_df_sorted_f1)), comparison_df_sorted_f1['F1-Score'], color=colors, alpha=0.8, edgecolor='black')\n",
    "            ax.set_yticks(range(len(comparison_df_sorted_f1)))\n",
    "            ax.set_yticklabels(comparison_df_sorted_f1['Approach'], fontsize=9)\n",
    "            ax.set_xlabel('F1-Score', fontsize=12, fontweight='bold')\n",
    "            ax.set_title('F1-Score Comparison', fontsize=12, fontweight='bold')\n",
    "            ax.grid(axis='x', alpha=0.3)\n",
    "            for i, v in enumerate(comparison_df_sorted_f1['F1-Score']):\n",
    "                if not pd.isna(v):\n",
    "                    ax.text(v, i, f'{v:.4f}', va='center', fontweight='bold', fontsize=9)\n",
    "            ax.invert_yaxis()\n",
    "        else:\n",
    "            ax.text(0.5, 0.5, 'No valid data', ha='center', va='center', transform=ax.transAxes)\n",
    "            ax.set_title('F1-Score Comparison', fontsize=12, fontweight='bold')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        print(\"âœ“ Visualizations complete!\")\n",
    "else:\n",
    "    print(\"âš  Warning: Cannot create visualizations without comparison data.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a9aa4ae",
   "metadata": {},
   "source": [
    "## 8. Save Mitigated Models and Results\n",
    "\n",
    "Save all mitigated models, predictions, and comparison results for final analysis in notebook 05.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df6d0538",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"SAVING MITIGATED MODELS AND RESULTS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Ensure results directory exists\n",
    "results_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Save mitigated models\n",
    "print(\"\\nSaving mitigated models...\")\n",
    "models_saved = 0\n",
    "for model_name, model_obj in mitigated_models.items():\n",
    "    try:\n",
    "        filename = f\"{model_name.lower().replace(' ', '_').replace('-', '_')}_model.pkl\"\n",
    "        filepath = results_dir / filename\n",
    "        with open(filepath, 'wb') as f:\n",
    "            pickle.dump(model_obj, f)\n",
    "        print(f\"  âœ“ Saved: {filename}\")\n",
    "        models_saved += 1\n",
    "    except Exception as e:\n",
    "        print(f\"  âš  Warning: Could not save {model_name}: {type(e).__name__}\")\n",
    "\n",
    "print(f\"\\nâœ“ Saved {models_saved} mitigated model(s)\")\n",
    "\n",
    "# Prepare results for JSON serialization\n",
    "def safe_convert(value):\n",
    "    \"\"\"Convert value to JSON-serializable format.\"\"\"\n",
    "    if pd.isna(value) or (isinstance(value, float) and np.isnan(value)):\n",
    "        return None\n",
    "    try:\n",
    "        return float(value)\n",
    "    except (TypeError, ValueError):\n",
    "        return str(value)\n",
    "\n",
    "# Save mitigation results\n",
    "print(\"\\nSaving mitigation results...\")\n",
    "mitigation_results_serializable = {}\n",
    "for approach_name, attr_results in mitigation_results.items():\n",
    "    mitigation_results_serializable[approach_name] = {}\n",
    "    for attr_name, metrics in attr_results.items():\n",
    "        mitigation_results_serializable[approach_name][attr_name] = {\n",
    "            k: safe_convert(v) for k, v in metrics.items()\n",
    "        }\n",
    "\n",
    "with open(results_dir / 'mitigation_results.json', 'w') as f:\n",
    "    json.dump(mitigation_results_serializable, f, indent=2)\n",
    "print(\"  âœ“ Saved: mitigation_results.json\")\n",
    "\n",
    "# Save comparison table\n",
    "if len(comparison_df) > 0:\n",
    "    comparison_df.to_csv(results_dir / 'mitigation_comparison.csv', index=False)\n",
    "    print(\"  âœ“ Saved: mitigation_comparison.csv\")\n",
    "\n",
    "# Save comprehensive comparison including baselines\n",
    "all_results_serializable = {}\n",
    "for approach_name, attr_results in all_results.items():\n",
    "    all_results_serializable[approach_name] = {}\n",
    "    for attr_name, metrics in attr_results.items():\n",
    "        all_results_serializable[approach_name][attr_name] = {\n",
    "            k: safe_convert(v) for k, v in metrics.items()\n",
    "        }\n",
    "\n",
    "with open(results_dir / 'all_models_comparison.json', 'w') as f:\n",
    "    json.dump(all_results_serializable, f, indent=2)\n",
    "print(\"  âœ“ Saved: all_models_comparison.json\")\n",
    "\n",
    "print(f\"\\nâœ“ All files saved to: {results_dir.absolute()}\")\n",
    "\n",
    "print(\"\\nSaved files:\")\n",
    "print(\"  Models:\")\n",
    "for model_name in mitigated_models.keys():\n",
    "    filename = f\"{model_name.lower().replace(' ', '_').replace('-', '_')}_model.pkl\"\n",
    "    print(f\"    - {filename}\")\n",
    "print(\"  Results:\")\n",
    "print(\"    - mitigation_results.json\")\n",
    "if len(comparison_df) > 0:\n",
    "    print(\"    - mitigation_comparison.csv\")\n",
    "print(\"    - all_models_comparison.json\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"NOTEBOOK COMPLETE!\")\n",
    "print(\"=\" * 60)\n",
    "print(\"Next steps:\")\n",
    "print(\"  â†’ Proceed to notebook 05_comparison.ipynb\")\n",
    "print(\"  â†’ Load comparison results and create final analysis\")\n",
    "print(\"  â†’ Generate final report with recommendations\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
