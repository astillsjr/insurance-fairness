{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ae66213",
   "metadata": {},
   "source": [
    "# 00: Exploratory Data Analysis\n",
    "\n",
    "This notebook provides an initial exploration of the auto insurance dataset to:\n",
    "- Understand data quality and completeness\n",
    "- Identify demographic distributions and potential bias patterns\n",
    "- Explore relationships between features and the target variable\n",
    "- Inform preprocessing decisions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f6e6f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies (for Google Colab)\n",
    "!pip install fairlearn seaborn -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be42272",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "\n",
    "# Set style for better visualizations\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Create results directory if it doesn't exist (for saving visualizations)\n",
    "Path('../results').mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eda00cd",
   "metadata": {},
   "source": [
    "## 1. Setup & Load Data\n",
    "\n",
    "### 1.1 Google Drive Mount (Optional - for Colab only)\n",
    "If running in Google Colab and data is stored in Google Drive, uncomment the following cell.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affce9a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment the following lines if you need to mount Google Drive in Colab\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6722650e",
   "metadata": {},
   "source": [
    "### 1.2 Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c73df857",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "# Option 1: Load from local directory (works for both local and Colab after upload)\n",
    "try:\n",
    "    file_path = '../data/AutoInsurance.csv'\n",
    "    df = pd.read_csv(file_path)\n",
    "    print(f\"✓ Successfully loaded data from {file_path}\")\n",
    "except FileNotFoundError:\n",
    "    # Option 2: Load from Colab content directory (after manual upload)\n",
    "    try:\n",
    "        file_path = '/content/AutoInsurance.csv'\n",
    "        df = pd.read_csv(file_path)\n",
    "        print(f\"✓ Successfully loaded data from {file_path}\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: The file was not found.\")\n",
    "        print(\"For local: Ensure '../data/AutoInsurance.csv' exists\")\n",
    "        print(\"For Colab: Upload the file using the folder icon in the left sidebar\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while loading the CSV: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4067eeb2",
   "metadata": {},
   "source": [
    "### 1.3 Data Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09946fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Inspection - Initial overview\n",
    "if 'df' in locals():\n",
    "    print(\"=\" * 60)\n",
    "    print(\"DATASET OVERVIEW\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"\\nShape of the DataFrame: {df.shape}\")\n",
    "    print(f\"Total records: {len(df):,}\")\n",
    "    print(f\"Total columns: {len(df.columns)}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"COLUMN NAMES:\")\n",
    "    print(\"=\" * 60)\n",
    "    print(df.columns.tolist())\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"DATA TYPES AND NON-NULL VALUES:\")\n",
    "    print(\"=\" * 60)\n",
    "    df.info()\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"FIRST FEW ROWS:\")\n",
    "    print(\"=\" * 60)\n",
    "    display(df.head(10))\n",
    "else:\n",
    "    print(\"DataFrame 'df' not created. Please check the previous steps for file loading errors.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d43f65e0",
   "metadata": {},
   "source": [
    "## 2. Data Quality Assessment\n",
    "\n",
    "### 2.1 Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05f05aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "if 'df' in locals():\n",
    "    missing_values = df.isnull().sum()\n",
    "    missing_percentage = (df.isnull().sum() / len(df)) * 100\n",
    "    missing_df = pd.DataFrame({\n",
    "        'Missing Count': missing_values, \n",
    "        'Missing Percentage (%)': missing_percentage\n",
    "    })\n",
    "    # Filter to show only columns with missing values, and sort them\n",
    "    missing_df = missing_df[missing_df['Missing Count'] > 0].sort_values(by='Missing Count', ascending=False)\n",
    "    \n",
    "    if not missing_df.empty:\n",
    "        print(\"\\nMissing Values and their Percentages:\")\n",
    "        display(missing_df)\n",
    "    else:\n",
    "        print(\"✓ No missing values found in the DataFrame.\")\n",
    "else:\n",
    "    print(\"DataFrame 'df' not created. Cannot check for missing values.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b09a381",
   "metadata": {},
   "source": [
    "### 2.2 Duplicate Records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7360af11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicate records\n",
    "if 'df' in locals():\n",
    "    num_duplicates = df.duplicated().sum()\n",
    "    if num_duplicates > 0:\n",
    "        print(f\"⚠ Found {num_duplicates} duplicate record(s).\")\n",
    "        print(\"Displaying the first 5 duplicate rows:\")\n",
    "        display(df[df.duplicated(keep=False)].sort_values(by=df.columns.tolist()).head())\n",
    "    else:\n",
    "        print(\"✓ No duplicate records found.\")\n",
    "else:\n",
    "    print(\"DataFrame 'df' not created. Cannot check for duplicates.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b53ec562",
   "metadata": {},
   "source": [
    "### 2.3 Outliers\n",
    "\n",
    "Examine numerical columns for potential outliers using descriptive statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f4f80d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for outliers in key numerical columns\n",
    "if 'df' in locals():\n",
    "    numerical_cols_for_outliers = ['Income', 'Total Claim Amount', 'Customer Lifetime Value']\n",
    "    print(f\"Descriptive Statistics for Outlier Identification:\")\n",
    "    for col in numerical_cols_for_outliers:\n",
    "        if col in df.columns:\n",
    "            print(f\"\\n{'='*60}\")\n",
    "            print(f\"Column: {col}\")\n",
    "            print(f\"{'='*60}\")\n",
    "            display(df[col].describe())\n",
    "            \n",
    "            # Box plot for visual outlier detection\n",
    "            plt.figure(figsize=(8, 5))\n",
    "            plt.boxplot(df[col].dropna())\n",
    "            plt.title(f'Box Plot: {col}')\n",
    "            plt.ylabel(col)\n",
    "            plt.show()\n",
    "        else:\n",
    "            print(f\"Column '{col}' not found in DataFrame.\")\n",
    "else:\n",
    "    print(\"DataFrame 'df' not created. Cannot check for outliers.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07fd6caf",
   "metadata": {},
   "source": [
    "### 2.4 Data Types Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a08342",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify data types and identify numerical vs categorical columns\n",
    "if 'df' in locals():\n",
    "    numerical_cols = df.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "    categorical_cols = df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "    \n",
    "    print(\"Numerical Columns:\")\n",
    "    print(numerical_cols)\n",
    "    print(f\"\\nTotal: {len(numerical_cols)} numerical columns\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Categorical Columns:\")\n",
    "    print(categorical_cols)\n",
    "    print(f\"\\nTotal: {len(categorical_cols)} categorical columns\")\n",
    "else:\n",
    "    print(\"DataFrame 'df' not created. Cannot verify data types.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c70d9775",
   "metadata": {},
   "source": [
    "### 2.5 Unique Values (Cardinality) of Categorical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef9643a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check cardinality of categorical features\n",
    "if 'df' in locals():\n",
    "    categorical_cols = df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "    print(\"Unique values (Cardinality) for Categorical Columns:\")\n",
    "    print(\"=\"*60)\n",
    "    for col in categorical_cols:\n",
    "        unique_count = df[col].nunique()\n",
    "        print(f\"\\n{col}: {unique_count} unique values\")\n",
    "        # Display actual unique values if there are few\n",
    "        if unique_count < 20:\n",
    "            print(f\"  Values: {df[col].unique().tolist()}\")\n",
    "        elif unique_count == len(df):\n",
    "            print(f\"  ⚠ Note: This appears to be a unique identifier (one value per row)\")\n",
    "else:\n",
    "    print(\"DataFrame 'df' not created. Cannot check unique values.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7da0f59",
   "metadata": {},
   "source": [
    "## 3. Target Variable Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "666c8eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine target variable (Response)\n",
    "target_col = 'Response'\n",
    "\n",
    "if 'df' in locals() and target_col in df.columns:\n",
    "    print(\"=\"*60)\n",
    "    print(\"TARGET VARIABLE DISTRIBUTION\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    response_distribution = df[target_col].value_counts()\n",
    "    print(\"\\nCount distribution:\")\n",
    "    display(response_distribution)\n",
    "    \n",
    "    response_percentage = df[target_col].value_counts(normalize=True) * 100\n",
    "    print(\"\\nPercentage distribution:\")\n",
    "    display(response_percentage)\n",
    "    \n",
    "    # Calculate class imbalance ratio\n",
    "    if len(response_distribution) == 2:\n",
    "        imbalance_ratio = response_distribution.max() / response_distribution.min()\n",
    "        print(f\"\\nClass Imbalance Ratio (Majority to Minority): {imbalance_ratio:.2f}\")\n",
    "        if imbalance_ratio > 5:\n",
    "            print(\"⚠ Warning: Significant class imbalance detected. Consider using class weights or resampling.\")\n",
    "    \n",
    "    # Enhanced visualization\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Count plot\n",
    "    sns.countplot(x=target_col, data=df, palette='viridis', ax=axes[0])\n",
    "    axes[0].set_title('Distribution of Response Variable (Count)')\n",
    "    axes[0].set_ylabel('Number of Customers')\n",
    "    axes[0].set_xlabel('Response')\n",
    "    \n",
    "    # Pie chart for percentages\n",
    "    response_counts = df[target_col].value_counts()\n",
    "    axes[1].pie(response_counts.values, labels=response_counts.index, autopct='%1.1f%%', \n",
    "                startangle=90, colors=['#440154', '#21918c'])\n",
    "    axes[1].set_title('Response Distribution (Percentages)')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Optional: Save visualization (uncomment to save)\n",
    "    # fig.savefig('../results/target_variable_distribution.png', dpi=300, bbox_inches='tight')\n",
    "    # print(\"\\n✓ Visualization saved to '../results/target_variable_distribution.png'\")\n",
    "    \n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"DataFrame 'df' not created or 'Response' column not found. Cannot perform target variable analysis.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "151fd56a",
   "metadata": {},
   "source": [
    "## 4. Protected Attributes Identification\n",
    "\n",
    "Identify and visualize demographic/socioeconomic attributes that may be sources of bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "806ff55b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define protected attributes for fairness analysis\n",
    "protected_attributes_categorical = [\n",
    "    'Gender',                   # M/F - direct protected attribute\n",
    "    'EmploymentStatus',         # Employed/Unemployed/etc. - socioeconomic status\n",
    "    'Education',                # Educational attainment - socioeconomic\n",
    "    'Marital Status',           # Marital status\n",
    "    'Location Code',            # Urban/Suburban/Rural - potential proxy for SES\n",
    "    'State'                    \n",
    "]\n",
    "protected_attributes_numerical = [\n",
    "    'Income',\n",
    "]\n",
    "\n",
    "# Visualize distributions of categorical protected attributes\n",
    "if 'df' in locals():\n",
    "    print(\"=\"*60)\n",
    "    print(\"PROTECTED ATTRIBUTES: CATEGORICAL DISTRIBUTIONS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    for col in protected_attributes_categorical:\n",
    "        if col in df.columns:\n",
    "            plt.figure(figsize=(8, 5))\n",
    "            sns.countplot(x=col, data=df, palette='viridis', order=df[col].value_counts().index)\n",
    "            plt.title(f'Distribution of {col}')\n",
    "            plt.xlabel(col)\n",
    "            plt.ylabel('Count')\n",
    "            plt.xticks(rotation=45, ha='right')\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "        else:\n",
    "            print(f\"Column '{col}' not found in DataFrame.\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"PROTECTED ATTRIBUTES: NUMERICAL DISTRIBUTIONS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    for col in protected_attributes_numerical:\n",
    "        if col in df.columns:\n",
    "            plt.figure(figsize=(8, 5))\n",
    "            sns.histplot(df[col], bins=30, kde=True, color='skyblue')\n",
    "            plt.title(f'Distribution of {col}')\n",
    "            plt.xlabel(col)\n",
    "            plt.ylabel('Frequency')\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "        else:\n",
    "            print(f\"Column '{col}' not found in DataFrame.\")\n",
    "else:\n",
    "    print(\"DataFrame 'df' not created. Cannot visualize protected attributes.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f46dfb2",
   "metadata": {},
   "source": [
    "## 5. Cross-Tabulation Analysis\n",
    "\n",
    "Analyze relationships between protected attributes and the target variable to identify potential bias patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f15495c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-tabulation analysis for protected attributes vs Response\n",
    "if 'df' in locals():\n",
    "    # Safety check: ensure protected attributes are defined\n",
    "    if 'protected_attributes_categorical' not in locals():\n",
    "        protected_attributes_categorical = ['Gender', 'EmploymentStatus', 'Education', 'Marital Status', 'Location Code', 'State']\n",
    "    if 'protected_attributes_numerical' not in locals():\n",
    "        protected_attributes_numerical = ['Income']\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(\"CROSS-TABULATION ANALYSIS: Protected Attributes vs Response\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Analyze categorical protected attributes\n",
    "    for col in protected_attributes_categorical:\n",
    "        if col in df.columns:\n",
    "            print(f\"\\n{'='*60}\")\n",
    "            print(f\"Analysis for {col}\")\n",
    "            print(f\"{'='*60}\")\n",
    "            \n",
    "            # Create crosstab\n",
    "            crosstab_df = pd.crosstab(df[col], df['Response'], normalize='index') * 100\n",
    "            print(f\"Response Rates by {col} (%):\")\n",
    "            display(crosstab_df)\n",
    "            \n",
    "            # Visualize with stacked bar chart\n",
    "            crosstab_df.plot(kind='bar', stacked=True, figsize=(10, 6), colormap='viridis')\n",
    "            plt.title(f'Response Rates by {col}')\n",
    "            plt.xlabel(col)\n",
    "            plt.ylabel('Percentage')\n",
    "            plt.xticks(rotation=45, ha='right')\n",
    "            plt.legend(title='Response')\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "        else:\n",
    "            print(f\"Column '{col}' not found in DataFrame. Skipping.\")\n",
    "    \n",
    "    # Handle numerical protected attribute 'Income' by binning\n",
    "    if 'Income' in df.columns:\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(\"Analysis for Income (Binned)\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        # Create income bins\n",
    "        df['Income_Binned_Labels'] = pd.cut(df['Income'], bins=5, include_lowest=True).astype(str)\n",
    "        \n",
    "        # Create crosstab for binned income\n",
    "        crosstab_income_df = pd.crosstab(df['Income_Binned_Labels'], df['Response'], normalize='index') * 100\n",
    "        print(\"Response Rates by Income Group (%):\")\n",
    "        display(crosstab_income_df)\n",
    "        \n",
    "        # Visualize\n",
    "        crosstab_income_df.plot(kind='bar', stacked=True, figsize=(10, 6), colormap='viridis')\n",
    "        plt.title('Response Rates by Income Group')\n",
    "        plt.xlabel('Income Group')\n",
    "        plt.ylabel('Percentage')\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        plt.legend(title='Response')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Drop the temporary binned income column\n",
    "        df.drop(columns=['Income_Binned_Labels'], inplace=True)\n",
    "    else:\n",
    "        print(\"Column 'Income' not found in DataFrame. Skipping.\")\n",
    "else:\n",
    "    print(\"DataFrame 'df' not created. Cannot perform cross-tabulation analysis.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b6b1f94",
   "metadata": {},
   "source": [
    "## 6. Feature-Target Relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a951d50d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature-Target Relationship Analysis\n",
    "if 'df' in locals():\n",
    "    # Convert 'Response' to numerical (0 for No, 1 for Yes) for correlation analysis\n",
    "    df_encoded = df.copy()\n",
    "    df_encoded['Response_Numerical'] = df_encoded['Response'].map({'No': 0, 'Yes': 1})\n",
    "    \n",
    "    numerical_cols = df_encoded.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "    # Ensure 'Response_Numerical' is in the list\n",
    "    if 'Response_Numerical' not in numerical_cols:\n",
    "        numerical_cols.append('Response_Numerical')\n",
    "    \n",
    "    # Calculate correlations with 'Response_Numerical'\n",
    "    relevant_numerical_cols = [col for col in numerical_cols if col in df_encoded.columns]\n",
    "    correlations = df_encoded[relevant_numerical_cols].corr()['Response_Numerical'].sort_values(ascending=False)\n",
    "    \n",
    "    print(\"=\"*60)\n",
    "    print(\"CORRELATION OF NUMERICAL FEATURES WITH RESPONSE\")\n",
    "    print(\"=\"*60)\n",
    "    display(correlations)\n",
    "    \n",
    "    # Visualize correlations with a heatmap\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.heatmap(df_encoded[relevant_numerical_cols].corr(), annot=True, cmap='coolwarm', \n",
    "                fmt=\".2f\", center=0, vmin=-1, vmax=1)\n",
    "    plt.title('Correlation Matrix of Numerical Features and Response')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Optional: Save visualization (uncomment to save)\n",
    "    # plt.savefig('../results/correlation_heatmap.png', dpi=300, bbox_inches='tight')\n",
    "    # print(\"\\n✓ Visualization saved to '../results/correlation_heatmap.png'\")\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"FEATURES WITH POTENTIALLY STRONG PREDICTIVE POWER\")\n",
    "    print(\"(Absolute correlation > 0.1):\")\n",
    "    print(\"=\"*60)\n",
    "    strong_predictors = correlations[abs(correlations) > 0.1]\n",
    "    display(strong_predictors)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"POTENTIAL PROXIES FOR PROTECTED ATTRIBUTES\")\n",
    "    print(\"(Correlation with Income):\")\n",
    "    print(\"=\"*60)\n",
    "    if 'Income' in relevant_numerical_cols:\n",
    "        income_correlations = df_encoded[relevant_numerical_cols].corr()['Income'].sort_values(ascending=False)\n",
    "        display(income_correlations[abs(income_correlations) > 0.2])  # Show only moderate+ correlations\n",
    "        print(\"\\n⚠ Note: Features strongly correlated with Income may act as proxies for socioeconomic status.\")\n",
    "else:\n",
    "    print(\"DataFrame 'df' not created. Cannot perform feature-target relationship analysis.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a2702d4",
   "metadata": {},
   "source": [
    "## 7. Summary Statistics\n",
    "\n",
    "Overall descriptive statistics for numerical and categorical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33debfae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics for numerical and categorical features\n",
    "if 'df' in locals():\n",
    "    print(\"=\"*60)\n",
    "    print(\"SUMMARY STATISTICS FOR NUMERICAL FEATURES\")\n",
    "    print(\"=\"*60)\n",
    "    display(df.describe())\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"SUMMARY STATISTICS FOR CATEGORICAL FEATURES\")\n",
    "    print(\"=\"*60)\n",
    "    categorical_cols = df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "    \n",
    "    # Create summary table for categorical features\n",
    "    categorical_summary = []\n",
    "    for col in categorical_cols:\n",
    "        value_counts = df[col].value_counts()\n",
    "        categorical_summary.append({\n",
    "            'Column': col,\n",
    "            'Unique Values': df[col].nunique(),\n",
    "            'Most Frequent': value_counts.index[0] if len(value_counts) > 0 else None,\n",
    "            'Most Frequent Count': value_counts.iloc[0] if len(value_counts) > 0 else 0,\n",
    "            'Most Frequent %': (value_counts.iloc[0] / len(df) * 100) if len(value_counts) > 0 else 0\n",
    "        })\n",
    "    \n",
    "    categorical_summary_df = pd.DataFrame(categorical_summary)\n",
    "    display(categorical_summary_df)\n",
    "else:\n",
    "    print(\"DataFrame 'df' not created. Cannot generate summary statistics.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5db4c13",
   "metadata": {},
   "source": [
    "## 8. Key Findings & Preprocessing Recommendations\n",
    "\n",
    "This section summarizes the key findings from the exploratory data analysis and provides recommendations for preprocessing.\n",
    "\n",
    "### Data Quality Report\n",
    "\n",
    "*   **Missing Values**: No missing values were found in the dataset, which simplifies the initial data cleaning process.\n",
    "*   **Duplicate Records**: No duplicate records were found, indicating a clean dataset in terms of unique entries.\n",
    "*   **Outliers**:\n",
    "    *   **Income**: Contains a significant number of zero values (approximately 25%), which heavily skews its distribution. There are also high-income outliers.\n",
    "    *   **Total Claim Amount**: Shows a right-skewed distribution with potential outliers on the higher end.\n",
    "    *   **Customer Lifetime Value**: Exhibits a highly right-skewed distribution with a long tail, suggesting many high-value outliers.\n",
    "    These outliers will need careful consideration (e.g., capping, transformation, or robust modeling techniques) during preprocessing.\n",
    "*   **Data Types**: Verified as mostly appropriate.\n",
    "    *   8 numerical columns (`float64`, `int64`)\n",
    "    *   16 object (categorical) columns.\n",
    "    *   'Effective To Date' is currently an `object` type and needs to be converted to `datetime` for proper temporal feature engineering.\n",
    "*   **Cardinality**:\n",
    "    *   'Customer' column has 9134 unique values, indicating it is a unique identifier. This column should be dropped as it provides no predictive power for the model.\n",
    "    *   Other categorical columns have a reasonable number of unique values suitable for encoding.\n",
    "\n",
    "### Demographic Imbalances Observed\n",
    "\n",
    "Analysis of protected attributes revealed several demographic imbalances:\n",
    "\n",
    "*   **Gender**: The distribution between 'Female' (F) and 'Male' (M) is relatively balanced.\n",
    "*   **EmploymentStatus**: Heavily skewed towards 'Employed' (approx. 62%) and 'Unemployed' (approx. 25%), with 'Medical Leave', 'Disabled', and 'Retired' groups being significantly smaller.\n",
    "*   **Education**: Skewed towards 'Bachelor' and 'College' degrees, with fewer individuals in 'High School or Below', 'Master', and 'Doctor' categories.\n",
    "*   **Marital Status**: 'Married' individuals constitute the largest group (approx. 58%), followed by 'Single' and 'Divorced'.\n",
    "*   **Location Code**: Predominantly 'Suburban' (approx. 63%), with 'Rural' and 'Urban' areas having fewer representatives.\n",
    "*   **State**: 'California' and 'Oregon' are the most represented states, while 'Washington' and 'Nevada' have smaller populations in the dataset.\n",
    "*   **Income**: The distribution is highly skewed, with a large concentration of zero incomes and a long tail extending to higher values.\n",
    "\n",
    "These imbalances highlight the need for careful evaluation of model performance across different groups to ensure fairness.\n",
    "\n",
    "### Initial Bias Patterns Identified (Cross-Tabulation Analysis with 'Response')\n",
    "\n",
    "The cross-tabulation analysis of protected attributes against the 'Response' (target) variable revealed initial patterns that could indicate bias:\n",
    "\n",
    "*   **EmploymentStatus**: 'Employed' and 'Retired' groups show noticeably higher 'Yes' response rates (approx. 17.7% and 16.9% respectively) compared to 'Unemployed' individuals (approx. 5.6%). This is a significant disparity that warrants further investigation for potential bias.\n",
    "*   **Education**: Individuals with 'Doctor', 'Bachelor', and 'College' degrees tend to have higher 'Yes' response rates (15.5% - 17.3%) than those with 'High School or Below' (approx. 9.5%).\n",
    "*   **Income**: Binned income analysis indicates that middle-income groups (e.g., (19996, 39992] and (59988, 79984]) have higher 'Yes' response rates (approx. 17.9% and 16.0%) compared to the lowest income bin (e.g., zero incomes) (approx. 12.6%) and the highest income bin (approx. 11.1%). The group with zero income (part of the lowest bin) has a particularly low response rate.\n",
    "*   **Gender**: 'Male' customers show a slightly higher 'Yes' response rate (approx. 14.9%) compared to 'Female' customers (approx. 13.7%). While small, this difference should be monitored.\n",
    "*   **Location Code**: 'Suburban' customers have a slightly higher 'Yes' response rate (approx. 15.1%) compared to 'Urban' (approx. 12.2%) and 'Rural' (approx. 13.3%) customers.\n",
    "*   **State** and **Marital Status**: Differences in response rates across these attributes were less pronounced but still present.\n",
    "\n",
    "These patterns suggest that certain demographic groups are more likely to respond 'Yes', which could lead to biased outcomes if not addressed.\n",
    "\n",
    "### Feature-Target Relationships\n",
    "\n",
    "*   **Numerical Features**: The correlation analysis between numerical features and the 'Response' target variable (converted to numerical) showed very weak linear relationships. The highest absolute correlations were around 0.01-0.02 (e.g., 'Total Claim Amount', 'Income', 'Monthly Premium Auto'). This indicates that individual numerical features, in isolation, are not strong linear predictors of the 'Response'.\n",
    "*   **Potential Proxy Relationships**: 'Income' showed a moderate negative correlation with 'Total Claim Amount' (-0.35). While not a direct proxy for protected groups in the traditional sense, this inverse relationship is noteworthy. Further analysis might be needed to understand if 'Total Claim Amount' can indirectly reflect socioeconomic status or contribute to disparate impact.\n",
    "\n",
    "### Preprocessing Recommendations\n",
    "\n",
    "Based on the EDA, the following preprocessing steps are recommended:\n",
    "\n",
    "1.  **Target Variable ('Response') Imbalance**:\n",
    "    *   The 'Response' variable is highly imbalanced with a majority-to-minority ratio of **5.98** (85.7% 'No' vs 14.3% 'Yes').\n",
    "    *   **Recommendation**: Address this imbalance using techniques such as oversampling the minority class (e.g., SMOTE), undersampling the majority class, or using class weights in model training.\n",
    "    *   **Evaluation Metrics**: Prioritize metrics robust to imbalance, such as F1-score, Precision, Recall, and AUC-ROC, over simple accuracy.\n",
    "\n",
    "2.  **Categorical Feature Encoding**:\n",
    "    *   All categorical features (excluding 'Customer' and 'Effective To Date') will require encoding.\n",
    "    *   **Recommendation**: Use appropriate encoding strategies like One-Hot Encoding for nominal features and Label Encoding or Ordinal Encoding for ordinal features, based on further domain knowledge or experimentation.\n",
    "\n",
    "3.  **'Effective To Date' Column**:\n",
    "    *   Currently an `object` type.\n",
    "    *   **Recommendation**: Convert this column to `datetime` objects. Extract useful temporal features such as 'year', 'month', 'day of week', 'day of month', or 'days until policy expiration' (if policy expiration info is available/derivable), and then drop the original column.\n",
    "\n",
    "4.  **'Customer' Column**:\n",
    "    *   Identified as a unique identifier with no predictive value.\n",
    "    *   **Recommendation**: Drop this column from the dataset before model training.\n",
    "\n",
    "5.  **Outlier Treatment**:\n",
    "    *   **Recommendation**: For 'Customer Lifetime Value', 'Income', and 'Total Claim Amount', explore strategies like capping (e.g., at the 95th or 99th percentile) or applying transformations (e.g., log transformation) to mitigate the impact of extreme outliers. Robust scaling methods can also be considered. The zero-income values might need special handling (e.g., imputation or treated as a separate category) if they represent a distinct group.\n",
    "\n",
    "6.  **Fairness Considerations**:\n",
    "    *   The identified demographic imbalances and disparate response rates across protected attributes (especially 'EmploymentStatus', 'Education', and 'Income' groups) indicate potential fairness issues.\n",
    "    *   **Recommendation**: During model development, monitor fairness metrics (e.g., demographic parity, equal opportunity) across these protected groups. Consider using fair-aware machine learning techniques if biases persist in model predictions.\n",
    "\n",
    "**Next Steps:**\n",
    "- Proceed to `01_preprocessing.ipynb` for data cleaning and preparation, incorporating these recommendations."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
