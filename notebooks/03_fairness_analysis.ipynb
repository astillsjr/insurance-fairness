{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19ff276c",
   "metadata": {},
   "source": [
    "# 03: Comprehensive Fairness Analysis\n",
    "\n",
    "This notebook provides a comprehensive evaluation of baseline models for group-level disparities:\n",
    "- Demographic parity analysis across all protected attributes\n",
    "- Equalized odds assessment (TPR and FPR)\n",
    "- Precision parity evaluation\n",
    "- Disaggregated performance metrics by protected attributes\n",
    "- Fairlearn-based comprehensive analysis\n",
    "- Visualizations of fairness metrics for all models and attributes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ceb293d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if running in Google Colab\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "except ImportError:\n",
    "    IN_COLAB = False\n",
    "\n",
    "# Install packages (for Google Colab)\n",
    "if IN_COLAB:\n",
    "    !pip install scikit-learn matplotlib seaborn fairlearn -q\n",
    "\n",
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import json\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score, \n",
    "    roc_auc_score, confusion_matrix, roc_curve\n",
    ")\n",
    "from fairlearn.metrics import (\n",
    "    MetricFrame, demographic_parity_difference, demographic_parity_ratio,\n",
    "    equalized_odds_difference, equalized_odds_ratio,\n",
    "    false_positive_rate, false_negative_rate\n",
    ")\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Plotting setup\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (14, 8)\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"COMPREHENSIVE FAIRNESS ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "print(\"âœ“ Setup complete!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96da3fa1",
   "metadata": {},
   "source": [
    "## 1. Load Data, Models, and Predictions\n",
    "\n",
    "Load the preprocessed data, trained models, and predictions from previous notebooks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c40a23b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine data directory (works for both local and Colab)\n",
    "if IN_COLAB:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    \n",
    "    possible_paths = [\n",
    "        Path('/content/drive/MyDrive/6.3950_project/processed_data'),\n",
    "        Path('/content/drive/Shareddrives/Insurance Fairness/processed_data'),\n",
    "        Path('/content/drive/MyDrive/insurance-fairness/results'),\n",
    "        Path('../results')\n",
    "    ]\n",
    "else:\n",
    "    possible_paths = [\n",
    "        Path('../results'),\n",
    "        Path('./results')\n",
    "    ]\n",
    "\n",
    "results_dir = None\n",
    "for path in possible_paths:\n",
    "    if path.exists() and (path / 'X_test.pkl').exists():\n",
    "        results_dir = path\n",
    "        break\n",
    "\n",
    "if results_dir is None:\n",
    "    error_msg = (\n",
    "        \"Could not find results directory. Please run notebooks 01 and 02 first.\\n\"\n",
    "        f\"Tried paths: {', '.join([str(p) for p in possible_paths])}\"\n",
    "    )\n",
    "    raise FileNotFoundError(error_msg)\n",
    "\n",
    "print(f\"âœ“ Loading data from: {results_dir}\")\n",
    "\n",
    "# Load test data and protected attributes\n",
    "with open(results_dir / 'X_test.pkl', 'rb') as f: X_test = pickle.load(f)\n",
    "with open(results_dir / 'y_test.pkl', 'rb') as f: y_test = pickle.load(f)\n",
    "with open(results_dir / 'protected_test.pkl', 'rb') as f: protected_test = pickle.load(f)\n",
    "\n",
    "# Load models\n",
    "print(\"Loading models...\")\n",
    "with open(results_dir / 'logistic_regression_model.pkl', 'rb') as f:\n",
    "    lr_model = pickle.load(f)\n",
    "with open(results_dir / 'random_forest_model.pkl', 'rb') as f:\n",
    "    rf_model = pickle.load(f)\n",
    "\n",
    "# Load predictions\n",
    "print(\"Loading predictions...\")\n",
    "with open(results_dir / 'lr_test_predictions.pkl', 'rb') as f:\n",
    "    lr_test_pred = pickle.load(f)\n",
    "with open(results_dir / 'lr_test_proba.pkl', 'rb') as f:\n",
    "    lr_test_proba = pickle.load(f)\n",
    "with open(results_dir / 'rf_test_predictions.pkl', 'rb') as f:\n",
    "    rf_test_pred = pickle.load(f)\n",
    "with open(results_dir / 'rf_test_proba.pkl', 'rb') as f:\n",
    "    rf_test_proba = pickle.load(f)\n",
    "\n",
    "# Load metadata\n",
    "try:\n",
    "    with open(results_dir / 'preprocessing_metadata.json', 'r') as f:\n",
    "        metadata = json.load(f)\n",
    "    protected_attributes_list = metadata.get('protected_attributes', [])\n",
    "except (FileNotFoundError, json.JSONDecodeError, KeyError) as e:\n",
    "    # Fallback: infer from protected_test columns\n",
    "    print(f\"âš  Warning: Could not load metadata ({type(e).__name__}), inferring from protected_test\")\n",
    "    protected_attributes_list = protected_test.columns.tolist()\n",
    "\n",
    "# Validate data consistency\n",
    "print(\"\\nðŸ” Validating data consistency...\")\n",
    "assert len(y_test) == len(X_test), f\"Length mismatch: y_test ({len(y_test)}) vs X_test ({len(X_test)})\"\n",
    "assert len(protected_test) == len(X_test), f\"Length mismatch: protected_test ({len(protected_test)}) vs X_test ({len(X_test)})\"\n",
    "assert len(lr_test_pred) == len(y_test), f\"Length mismatch: lr_test_pred ({len(lr_test_pred)}) vs y_test ({len(y_test)})\"\n",
    "assert len(rf_test_pred) == len(y_test), f\"Length mismatch: rf_test_pred ({len(rf_test_pred)}) vs y_test ({len(y_test)})\"\n",
    "assert len(lr_test_proba) == len(y_test), f\"Length mismatch: lr_test_proba ({len(lr_test_proba)}) vs y_test ({len(y_test)})\"\n",
    "assert len(rf_test_proba) == len(y_test), f\"Length mismatch: rf_test_proba ({len(rf_test_proba)}) vs y_test ({len(y_test)})\"\n",
    "\n",
    "# Validate models loaded successfully\n",
    "assert hasattr(lr_model, 'predict'), \"LR model missing predict method\"\n",
    "assert hasattr(rf_model, 'predict'), \"RF model missing predict method\"\n",
    "print(\"âœ“ All data validated successfully!\")\n",
    "\n",
    "print(f\"\\nâœ“ Data loaded!\")\n",
    "print(f\"   Test set: {len(X_test):,} samples\")\n",
    "print(f\"   Protected attributes: {', '.join(protected_attributes_list)}\")\n",
    "print(f\"   Class distribution: {(y_test == 0).sum():,} No, {(y_test == 1).sum():,} Yes\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c709622",
   "metadata": {},
   "source": [
    "## 2. Helper Functions for Fairness Analysis\n",
    "\n",
    "Define reusable functions for calculating and visualizing fairness metrics.\n",
    "\n",
    "### 2.1 Understanding Fairness Metrics\n",
    "\n",
    "**Demographic Parity (Statistical Parity):**\n",
    "- Measures equal selection rates across groups\n",
    "- Formula: P(Å¶=1 | A=a) should be equal for all groups\n",
    "- Ideal difference: 0 (perfect parity)\n",
    "- Threshold: Differences > 0.05 (5 percentage points) indicate significant disparities\n",
    "\n",
    "**Equalized Odds:**\n",
    "- Measures equal true positive rates (TPR) and false positive rates (FPR) across groups\n",
    "- TPR: P(Å¶=1 | Y=1, A=a) - equal across groups\n",
    "- FPR: P(Å¶=1 | Y=0, A=a) - equal across groups\n",
    "- Ideal difference: 0 for both TPR and FPR\n",
    "- Ensures model performs equally well for all groups\n",
    "\n",
    "**Precision Parity:**\n",
    "- Measures equal precision (positive predictive value) across groups\n",
    "- Formula: P(Y=1 | Å¶=1, A=a) should be equal\n",
    "- Ideal difference: 0\n",
    "- Ensures equal confidence in positive predictions across groups\n",
    "\n",
    "**Interpretation:**\n",
    "- Lower differences indicate better fairness\n",
    "- Differences > 0.05 (5pp) are flagged as significant disparities\n",
    "- All metrics are calculated on the test set for unbiased evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17514f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_fairness_metrics(y_true, y_pred, sensitive_feature, model_name, attr_name):\n",
    "    \"\"\"\n",
    "    Calculate comprehensive fairness metrics for a given model and protected attribute.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    y_true : array-like\n",
    "        True labels\n",
    "    y_pred : array-like\n",
    "        Predicted labels\n",
    "    sensitive_feature : array-like\n",
    "        Protected attribute values\n",
    "    model_name : str\n",
    "        Name of the model\n",
    "    attr_name : str\n",
    "        Name of the protected attribute\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    dict : Dictionary containing all fairness metrics\n",
    "    \"\"\"\n",
    "    # Input validation\n",
    "    y_true = np.asarray(y_true)\n",
    "    y_pred = np.asarray(y_pred)\n",
    "    sensitive_feature = np.asarray(sensitive_feature)\n",
    "    \n",
    "    if len(y_true) != len(y_pred) or len(y_true) != len(sensitive_feature):\n",
    "        raise ValueError(f\"Length mismatch: y_true ({len(y_true)}), y_pred ({len(y_pred)}), sensitive_feature ({len(sensitive_feature)})\")\n",
    "    \n",
    "    # Check for edge case: all predictions are the same class\n",
    "    unique_preds = np.unique(y_pred)\n",
    "    if len(unique_preds) == 1:\n",
    "        print(f\"   âš  Warning: All predictions are class {unique_preds[0]}, some metrics may be undefined\")\n",
    "    # Use fairlearn MetricFrame for comprehensive metrics\n",
    "    metrics_dict = {\n",
    "        'accuracy': accuracy_score,\n",
    "        'precision': precision_score,\n",
    "        'recall': recall_score,\n",
    "        'f1': f1_score,\n",
    "        'false_positive_rate': false_positive_rate,\n",
    "        'false_negative_rate': false_negative_rate\n",
    "    }\n",
    "    \n",
    "    metric_frame = MetricFrame(\n",
    "        metrics=metrics_dict,\n",
    "        y_true=y_true,\n",
    "        y_pred=y_pred,\n",
    "        sensitive_features=sensitive_feature\n",
    "    )\n",
    "    \n",
    "    # Calculate group-level metrics\n",
    "    by_group = metric_frame.by_group\n",
    "    \n",
    "    # Calculate fairness metrics\n",
    "    dp_diff = demographic_parity_difference(y_true, y_pred, sensitive_features=sensitive_feature)\n",
    "    dp_ratio = demographic_parity_ratio(y_true, y_pred, sensitive_features=sensitive_feature)\n",
    "    eo_diff = equalized_odds_difference(y_true, y_pred, sensitive_features=sensitive_feature)\n",
    "    eo_ratio = equalized_odds_ratio(y_true, y_pred, sensitive_features=sensitive_feature)\n",
    "    \n",
    "    # Calculate precision parity (precision difference across groups)\n",
    "    # Handle edge case: if all predictions are the same class, precision may be NaN\n",
    "    precision_by_group = by_group['precision']\n",
    "    # Filter out NaN values for precision calculations\n",
    "    valid_precision = precision_by_group.dropna()\n",
    "    if len(valid_precision) > 0:\n",
    "        precision_diff = valid_precision.max() - valid_precision.min()\n",
    "        precision_ratio = valid_precision.min() / valid_precision.max() if valid_precision.max() > 0 else 0\n",
    "    else:\n",
    "        # All groups have undefined precision (likely all predictions are same class)\n",
    "        precision_diff = np.nan\n",
    "        precision_ratio = np.nan\n",
    "    \n",
    "    # Calculate selection rate (approval rate) by group\n",
    "    # Also validate group sizes\n",
    "    selection_by_group = {}\n",
    "    group_sizes = {}\n",
    "    min_group_size = 10  # Minimum samples per group for reliable metrics\n",
    "    \n",
    "    for group in np.unique(sensitive_feature):\n",
    "        mask = sensitive_feature == group\n",
    "        group_size = mask.sum()\n",
    "        group_sizes[group] = group_size\n",
    "        \n",
    "        if group_size < min_group_size:\n",
    "            print(f\"   âš  Warning: Group '{group}' has only {group_size} samples (< {min_group_size})\")\n",
    "        \n",
    "        selection_by_group[group] = y_pred[mask].mean()\n",
    "    \n",
    "    # Calculate TPR and FPR differences\n",
    "    # Handle edge cases where metrics might be NaN\n",
    "    tpr_by_group = by_group['recall']  # TPR = Recall\n",
    "    fpr_by_group = by_group['false_positive_rate']\n",
    "    \n",
    "    # Filter out NaN values\n",
    "    valid_tpr = tpr_by_group.dropna()\n",
    "    valid_fpr = fpr_by_group.dropna()\n",
    "    \n",
    "    if len(valid_tpr) > 0:\n",
    "        tpr_diff = valid_tpr.max() - valid_tpr.min()\n",
    "    else:\n",
    "        tpr_diff = np.nan\n",
    "    \n",
    "    if len(valid_fpr) > 0:\n",
    "        fpr_diff = valid_fpr.max() - valid_fpr.min()\n",
    "    else:\n",
    "        fpr_diff = np.nan\n",
    "    \n",
    "    return {\n",
    "        'model_name': model_name,\n",
    "        'attribute_name': attr_name,\n",
    "        'by_group': by_group,\n",
    "        'demographic_parity_difference': dp_diff,\n",
    "        'demographic_parity_ratio': dp_ratio,\n",
    "        'equalized_odds_difference': eo_diff,\n",
    "        'equalized_odds_ratio': eo_ratio,\n",
    "        'precision_difference': precision_diff,\n",
    "        'precision_ratio': precision_ratio,\n",
    "        'tpr_difference': tpr_diff,\n",
    "        'fpr_difference': fpr_diff,\n",
    "        'selection_by_group': selection_by_group,\n",
    "        'tpr_by_group': tpr_by_group.to_dict(),\n",
    "        'fpr_by_group': fpr_by_group.to_dict(),\n",
    "        'precision_by_group': precision_by_group.to_dict(),\n",
    "        'group_sizes': group_sizes\n",
    "    }\n",
    "\n",
    "\n",
    "def visualize_fairness_metrics(metrics_dict, model_name, attr_name):\n",
    "    \"\"\"\n",
    "    Create visualizations for fairness metrics.\n",
    "    \"\"\"\n",
    "    by_group = metrics_dict['by_group']\n",
    "    # Ensure groups match selection_by_group keys and are sorted for consistency\n",
    "    groups = sorted(list(metrics_dict['selection_by_group'].keys()))\n",
    "    \n",
    "    # Verify all groups exist in by_group\n",
    "    missing_groups = [g for g in groups if g not in by_group.index]\n",
    "    if missing_groups:\n",
    "        print(f\"âš  Warning: Groups {missing_groups} not found in by_group, skipping visualization\")\n",
    "        return\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "    fig.suptitle(f'{model_name} - {attr_name} Fairness Analysis', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # 1. Demographic Parity (Selection Rate)\n",
    "    selection_rates = [metrics_dict['selection_by_group'][g] for g in groups]\n",
    "    axes[0, 0].bar(groups, selection_rates, color='steelblue', alpha=0.8)\n",
    "    axes[0, 0].set_ylabel('Selection Rate', fontsize=11)\n",
    "    axes[0, 0].set_title(f'Demographic Parity\\n(Diff: {metrics_dict[\"demographic_parity_difference\"]:.4f})', \n",
    "                        fontsize=11, fontweight='bold')\n",
    "    axes[0, 0].grid(axis='y', alpha=0.3)\n",
    "    for i, (g, rate) in enumerate(zip(groups, selection_rates)):\n",
    "        axes[0, 0].text(i, rate, f'{rate:.2%}', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    # 2. Equalized Odds - TPR\n",
    "    tpr_values = [metrics_dict['tpr_by_group'][g] for g in groups]\n",
    "    axes[0, 1].bar(groups, tpr_values, color='forestgreen', alpha=0.8)\n",
    "    axes[0, 1].set_ylabel('True Positive Rate', fontsize=11)\n",
    "    axes[0, 1].set_title(f'Equalized Odds - TPR\\n(Diff: {metrics_dict[\"tpr_difference\"]:.4f})', \n",
    "                        fontsize=11, fontweight='bold')\n",
    "    axes[0, 1].grid(axis='y', alpha=0.3)\n",
    "    for i, (g, tpr) in enumerate(zip(groups, tpr_values)):\n",
    "        axes[0, 1].text(i, tpr, f'{tpr:.2%}', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    # 3. Equalized Odds - FPR\n",
    "    fpr_values = [metrics_dict['fpr_by_group'][g] for g in groups]\n",
    "    axes[0, 2].bar(groups, fpr_values, color='coral', alpha=0.8)\n",
    "    axes[0, 2].set_ylabel('False Positive Rate', fontsize=11)\n",
    "    axes[0, 2].set_title(f'Equalized Odds - FPR\\n(Diff: {metrics_dict[\"fpr_difference\"]:.4f})', \n",
    "                        fontsize=11, fontweight='bold')\n",
    "    axes[0, 2].grid(axis='y', alpha=0.3)\n",
    "    for i, (g, fpr) in enumerate(zip(groups, fpr_values)):\n",
    "        axes[0, 2].text(i, fpr, f'{fpr:.2%}', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    # 4. Precision Parity\n",
    "    precision_values = [metrics_dict['precision_by_group'][g] for g in groups]\n",
    "    axes[1, 0].bar(groups, precision_values, color='purple', alpha=0.8)\n",
    "    axes[1, 0].set_ylabel('Precision', fontsize=11)\n",
    "    axes[1, 0].set_title(f'Precision Parity\\n(Diff: {metrics_dict[\"precision_difference\"]:.4f})', \n",
    "                        fontsize=11, fontweight='bold')\n",
    "    axes[1, 0].grid(axis='y', alpha=0.3)\n",
    "    for i, (g, prec) in enumerate(zip(groups, precision_values)):\n",
    "        axes[1, 0].text(i, prec, f'{prec:.2%}', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    # 5. Accuracy by Group\n",
    "    accuracy_values = [by_group.loc[g, 'accuracy'] for g in groups]\n",
    "    axes[1, 1].bar(groups, accuracy_values, color='teal', alpha=0.8)\n",
    "    axes[1, 1].set_ylabel('Accuracy', fontsize=11)\n",
    "    axes[1, 1].set_title('Accuracy by Group', fontsize=11, fontweight='bold')\n",
    "    axes[1, 1].grid(axis='y', alpha=0.3)\n",
    "    for i, (g, acc) in enumerate(zip(groups, accuracy_values)):\n",
    "        axes[1, 1].text(i, acc, f'{acc:.2%}', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    # 6. F1-Score by Group\n",
    "    f1_values = [by_group.loc[g, 'f1'] for g in groups]\n",
    "    axes[1, 2].bar(groups, f1_values, color='orange', alpha=0.8)\n",
    "    axes[1, 2].set_ylabel('F1-Score', fontsize=11)\n",
    "    axes[1, 2].set_title('F1-Score by Group', fontsize=11, fontweight='bold')\n",
    "    axes[1, 2].grid(axis='y', alpha=0.3)\n",
    "    for i, (g, f1) in enumerate(zip(groups, f1_values)):\n",
    "        axes[1, 2].text(i, f1, f'{f1:.2%}', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "print(\"âœ“ Helper functions defined\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e50fdb2",
   "metadata": {},
   "source": [
    "## 3. Comprehensive Fairness Analysis\n",
    "\n",
    "Analyze both models across all protected attributes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afaa63f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store all fairness results\n",
    "all_fairness_results = {}\n",
    "\n",
    "# Models to analyze\n",
    "models = {\n",
    "    'Logistic Regression': (lr_test_pred, lr_test_proba),\n",
    "    'Random Forest': (rf_test_pred, rf_test_proba)\n",
    "}\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"COMPREHENSIVE FAIRNESS ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Analyze each model and each protected attribute\n",
    "for model_name, (y_pred, y_proba) in models.items():\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Analyzing: {model_name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    all_fairness_results[model_name] = {}\n",
    "    \n",
    "    for attr in protected_attributes_list:\n",
    "        if attr not in protected_test.columns:\n",
    "            print(f\"âš  Warning: {attr} not found in protected_test, skipping...\")\n",
    "            continue\n",
    "            \n",
    "        sensitive_feature = protected_test[attr]\n",
    "        \n",
    "        # Skip if all values are the same\n",
    "        if len(sensitive_feature.unique()) < 2:\n",
    "            print(f\"âš  Warning: {attr} has only one unique value, skipping...\")\n",
    "            continue\n",
    "        \n",
    "        print(f\"\\nðŸ“Š Analyzing protected attribute: {attr}\")\n",
    "        print(f\"   Groups: {sorted(sensitive_feature.unique())}\")\n",
    "        \n",
    "        # Calculate fairness metrics\n",
    "        metrics = calculate_fairness_metrics(\n",
    "            y_test, y_pred, sensitive_feature, model_name, attr\n",
    "        )\n",
    "        \n",
    "        all_fairness_results[model_name][attr] = metrics\n",
    "        \n",
    "        # Print summary\n",
    "        print(f\"\\n   âš–ï¸  Fairness Metrics:\")\n",
    "        print(f\"      Demographic Parity Difference: {metrics['demographic_parity_difference']:.4f}\")\n",
    "        print(f\"      Equalized Odds Difference: {metrics['equalized_odds_difference']:.4f}\")\n",
    "        print(f\"      Precision Difference: {metrics['precision_difference']:.4f}\")\n",
    "        print(f\"      TPR Difference: {metrics['tpr_difference']:.4f}\")\n",
    "        print(f\"      FPR Difference: {metrics['fpr_difference']:.4f}\")\n",
    "        \n",
    "        # Check for significant disparities\n",
    "        warnings = []\n",
    "        if metrics['demographic_parity_difference'] > 0.05:\n",
    "            warnings.append(f\"âš  Demographic parity disparity > 5pp\")\n",
    "        if metrics['equalized_odds_difference'] > 0.05:\n",
    "            warnings.append(f\"âš  Equalized odds disparity > 5pp\")\n",
    "        if metrics['precision_difference'] > 0.05:\n",
    "            warnings.append(f\"âš  Precision disparity > 5pp\")\n",
    "        \n",
    "        if warnings:\n",
    "            print(f\"\\n   {' '.join(warnings)}\")\n",
    "        \n",
    "        # Display group-level metrics\n",
    "        print(f\"\\n   ðŸ“ˆ Group-Level Performance:\")\n",
    "        print(metrics['by_group'].to_string())\n",
    "        \n",
    "        # Visualize\n",
    "        visualize_fairness_metrics(metrics, model_name, attr)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"âœ“ COMPREHENSIVE ANALYSIS COMPLETE\")\n",
    "print(\"=\" * 60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9fb8729",
   "metadata": {},
   "source": [
    "## 4. Summary Comparison\n",
    "\n",
    "Create summary tables comparing fairness metrics across models and attributes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b87c3de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive summary table\n",
    "summary_rows = []\n",
    "\n",
    "for model_name in all_fairness_results:\n",
    "    for attr_name in all_fairness_results[model_name]:\n",
    "        metrics = all_fairness_results[model_name][attr_name]\n",
    "        summary_rows.append({\n",
    "            'Model': model_name,\n",
    "            'Protected Attribute': attr_name,\n",
    "            'Demographic Parity Diff': metrics['demographic_parity_difference'],\n",
    "            'Equalized Odds Diff': metrics['equalized_odds_difference'],\n",
    "            'Precision Diff': metrics['precision_difference'],\n",
    "            'TPR Diff': metrics['tpr_difference'],\n",
    "            'FPR Diff': metrics['fpr_difference']\n",
    "        })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_rows)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"FAIRNESS METRICS SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\nAll differences are absolute differences between groups.\")\n",
    "print(\"Lower values indicate better fairness.\\n\")\n",
    "print(summary_df.round(4).to_string())\n",
    "\n",
    "# Create heatmap of fairness disparities\n",
    "# Check if we have enough data for heatmaps\n",
    "num_models = summary_df['Model'].nunique()\n",
    "num_attributes = summary_df['Protected Attribute'].nunique()\n",
    "\n",
    "if num_models >= 2 and num_attributes >= 2:\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "    \n",
    "    # Pivot tables for heatmaps\n",
    "    dp_pivot = summary_df.pivot(index='Protected Attribute', columns='Model', values='Demographic Parity Diff')\n",
    "    eo_pivot = summary_df.pivot(index='Protected Attribute', columns='Model', values='Equalized Odds Diff')\n",
    "    prec_pivot = summary_df.pivot(index='Protected Attribute', columns='Model', values='Precision Diff')\n",
    "    \n",
    "    # Demographic Parity Heatmap\n",
    "    sns.heatmap(dp_pivot, annot=True, fmt='.4f', cmap='Reds', ax=axes[0], cbar_kws={'label': 'Difference'})\n",
    "    axes[0].set_title('Demographic Parity Difference', fontsize=12, fontweight='bold')\n",
    "    \n",
    "    # Equalized Odds Heatmap\n",
    "    sns.heatmap(eo_pivot, annot=True, fmt='.4f', cmap='Oranges', ax=axes[1], cbar_kws={'label': 'Difference'})\n",
    "    axes[1].set_title('Equalized Odds Difference', fontsize=12, fontweight='bold')\n",
    "    \n",
    "    # Precision Parity Heatmap\n",
    "    sns.heatmap(prec_pivot, annot=True, fmt='.4f', cmap='Purples', ax=axes[2], cbar_kws={'label': 'Difference'})\n",
    "    axes[2].set_title('Precision Parity Difference', fontsize=12, fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(f\"âš  Cannot create heatmaps: Need at least 2 models and 2 attributes\")\n",
    "    print(f\"   Current: {num_models} model(s), {num_attributes} attribute(s)\")\n",
    "    print(\"   Displaying summary table instead (see above)\")\n",
    "\n",
    "print(\"\\nðŸ“Š Key Findings:\")\n",
    "# Handle NaN values in summary statistics\n",
    "dp_max = summary_df['Demographic Parity Diff'].max()\n",
    "eo_max = summary_df['Equalized Odds Diff'].max()\n",
    "prec_max = summary_df['Precision Diff'].max()\n",
    "\n",
    "print(f\"   â€¢ Highest Demographic Parity Disparity: {dp_max:.4f}\" if not pd.isna(dp_max) else \"   â€¢ Highest Demographic Parity Disparity: N/A (NaN values present)\")\n",
    "print(f\"   â€¢ Highest Equalized Odds Disparity: {eo_max:.4f}\" if not pd.isna(eo_max) else \"   â€¢ Highest Equalized Odds Disparity: N/A (NaN values present)\")\n",
    "print(f\"   â€¢ Highest Precision Disparity: {prec_max:.4f}\" if not pd.isna(prec_max) else \"   â€¢ Highest Precision Disparity: N/A (NaN values present)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b2e115",
   "metadata": {},
   "source": [
    "## 5. Save Fairness Analysis Results\n",
    "\n",
    "Save all fairness metrics for downstream comparison with mitigated models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac4713b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare results for saving (convert to JSON-serializable format)\n",
    "fairness_results_serializable = {}\n",
    "\n",
    "for model_name in all_fairness_results:\n",
    "    fairness_results_serializable[model_name] = {}\n",
    "    for attr_name in all_fairness_results[model_name]:\n",
    "        metrics = all_fairness_results[model_name][attr_name]\n",
    "        \n",
    "        # Convert by_group DataFrame to dict\n",
    "        by_group_dict = metrics['by_group'].to_dict('index')\n",
    "        \n",
    "        # Handle NaN values in serialization (convert to None for JSON compatibility)\n",
    "        def safe_float(value):\n",
    "            \"\"\"Convert value to float, or None if NaN.\"\"\"\n",
    "            try:\n",
    "                if pd.isna(value) or np.isnan(value):\n",
    "                    return None\n",
    "                return float(value)\n",
    "            except (TypeError, ValueError):\n",
    "                return None\n",
    "        \n",
    "        fairness_results_serializable[model_name][attr_name] = {\n",
    "            'demographic_parity_difference': safe_float(metrics['demographic_parity_difference']),\n",
    "            'demographic_parity_ratio': safe_float(metrics['demographic_parity_ratio']),\n",
    "            'equalized_odds_difference': safe_float(metrics['equalized_odds_difference']),\n",
    "            'equalized_odds_ratio': safe_float(metrics['equalized_odds_ratio']),\n",
    "            'precision_difference': safe_float(metrics['precision_difference']),\n",
    "            'precision_ratio': safe_float(metrics['precision_ratio']),\n",
    "            'tpr_difference': safe_float(metrics['tpr_difference']),\n",
    "            'fpr_difference': safe_float(metrics['fpr_difference']),\n",
    "            'by_group': by_group_dict,\n",
    "            'selection_by_group': {str(k): float(v) for k, v in metrics['selection_by_group'].items()},\n",
    "            'tpr_by_group': {str(k): float(v) for k, v in metrics['tpr_by_group'].items()},\n",
    "            'fpr_by_group': {str(k): float(v) for k, v in metrics['fpr_by_group'].items()},\n",
    "            'precision_by_group': {str(k): float(v) for k, v in metrics['precision_by_group'].items()},\n",
    "            'group_sizes': {str(k): int(v) for k, v in metrics.get('group_sizes', {}).items()}\n",
    "        }\n",
    "\n",
    "# Save to JSON\n",
    "with open(results_dir / 'baseline_fairness_analysis.json', 'w') as f:\n",
    "    json.dump(fairness_results_serializable, f, indent=2)\n",
    "\n",
    "# Save summary table as CSV\n",
    "summary_df.to_csv(results_dir / 'baseline_fairness_summary.csv', index=False)\n",
    "\n",
    "print(\"âœ“ Fairness analysis results saved!\")\n",
    "print(f\"   â€¢ baseline_fairness_analysis.json\")\n",
    "print(f\"   â€¢ baseline_fairness_summary.csv\")\n",
    "print(f\"\\n   Saved to: {results_dir.absolute()}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"NOTEBOOK COMPLETE!\")\n",
    "print(\"=\" * 60)\n",
    "print(\"Next steps:\")\n",
    "print(\"  â†’ Proceed to notebook 04_mitigation.ipynb\")\n",
    "print(\"  â†’ Apply fairness-aware techniques to reduce identified disparities\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
