{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f6aaa1b",
   "metadata": {},
   "source": [
    "# 02: Baseline Models\n",
    "\n",
    "This notebook trains and evaluates baseline machine learning models:\n",
    "- Logistic Regression (with proper scaling)\n",
    "- Random Forest\n",
    "- Model evaluation on train, validation, and test sets\n",
    "- Feature importance analysis\n",
    "- Save trained models and predictions for fairness analysis\n",
    "\n",
    "**Note:** Fairness analysis is performed separately in notebook 03_fairness_analysis.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f2595f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if running in Google Colab\n",
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "except ImportError:\n",
    "    IN_COLAB = False\n",
    "\n",
    "# Install packages (for Google Colab)\n",
    "if IN_COLAB:\n",
    "    !pip install scikit-learn matplotlib seaborn fairlearn -q\n",
    "\n",
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import json\n",
    "import time\n",
    "from pathlib import Path\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score,\n",
    "                             f1_score, roc_auc_score, confusion_matrix, roc_curve)\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Plotting setup\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"BASELINE MODELS - INSURANCE CLAIM PREDICTION\")\n",
    "print(\"=\" * 60)\n",
    "print(\"‚úì Setup complete!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "721f452d",
   "metadata": {},
   "source": [
    "## 1. Load Preprocessed Data\n",
    "\n",
    "Load the processed data from the preprocessing notebook. Works both locally and on Google Colab.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "227b737a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine data directory (works for both local and Colab)\n",
    "if IN_COLAB:\n",
    "    # Mount Google Drive for Colab\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    \n",
    "    # Try common Colab paths\n",
    "    possible_paths = [\n",
    "        Path('/content/drive/MyDrive/6.3950_project/processed_data'),\n",
    "        Path('/content/drive/Shareddrives/Insurance Fairness/processed_data'),\n",
    "        Path('/content/drive/MyDrive/insurance-fairness/results'),\n",
    "        Path('../results')\n",
    "    ]\n",
    "else:\n",
    "    # Local paths\n",
    "    possible_paths = [\n",
    "        Path('../results'),\n",
    "        Path('./results')\n",
    "    ]\n",
    "\n",
    "data_dir = None\n",
    "for path in possible_paths:\n",
    "    if path.exists() and (path / 'X_train.pkl').exists():\n",
    "        data_dir = path\n",
    "        break\n",
    "\n",
    "if data_dir is None:\n",
    "    error_msg = (\n",
    "        \"Could not find preprocessed data. Please run notebook 01_preprocessing.ipynb first.\\n\"\n",
    "        f\"Tried paths: {', '.join([str(p) for p in possible_paths])}\\n\"\n",
    "        \"\\nTroubleshooting tips:\\n\"\n",
    "        \"  - Ensure you've run 01_preprocessing.ipynb completely\\n\"\n",
    "        \"  - Check that the results/ directory exists\\n\"\n",
    "        \"  - For Colab: Verify Google Drive is mounted correctly\"\n",
    "    )\n",
    "    raise FileNotFoundError(error_msg)\n",
    "\n",
    "print(f\"‚úì Loading data from: {data_dir}\")\n",
    "print(\"Loading preprocessed data...\")\n",
    "\n",
    "# Load all data splits\n",
    "with open(data_dir / 'X_train.pkl', 'rb') as f: X_train = pickle.load(f)\n",
    "with open(data_dir / 'X_val.pkl', 'rb') as f: X_val = pickle.load(f)\n",
    "with open(data_dir / 'X_test.pkl', 'rb') as f: X_test = pickle.load(f)\n",
    "with open(data_dir / 'y_train.pkl', 'rb') as f: y_train = pickle.load(f)\n",
    "with open(data_dir / 'y_val.pkl', 'rb') as f: y_val = pickle.load(f)\n",
    "with open(data_dir / 'y_test.pkl', 'rb') as f: y_test = pickle.load(f)\n",
    "with open(data_dir / 'protected_train.pkl', 'rb') as f: protected_train = pickle.load(f)\n",
    "with open(data_dir / 'protected_val.pkl', 'rb') as f: protected_val = pickle.load(f)\n",
    "with open(data_dir / 'protected_test.pkl', 'rb') as f: protected_test = pickle.load(f)\n",
    "with open(data_dir / 'feature_names.pkl', 'rb') as f: feature_names = pickle.load(f)\n",
    "\n",
    "# Validate data shapes\n",
    "assert X_train.shape[1] == X_val.shape[1] == X_test.shape[1], \"Feature count mismatch\"\n",
    "assert len(y_train) == len(X_train), \"Label/feature count mismatch\"\n",
    "assert len(protected_train) == len(X_train), \"Protected attributes count mismatch\"\n",
    "\n",
    "# Load scaler (data is already scaled, but we keep it for consistency)\n",
    "try:\n",
    "    with open(data_dir / 'scaler.pkl', 'rb') as f: scaler = pickle.load(f)\n",
    "    print(\"‚úì Scaler loaded (data is already scaled)\")\n",
    "except FileNotFoundError:\n",
    "    print(\"‚ö† Scaler not found, but data should already be scaled\")\n",
    "    scaler = None\n",
    "\n",
    "print(\"\\n‚úÖ DATA LOADED!\")\n",
    "print(f\"   Training: {X_train.shape[0]:,} samples √ó {X_train.shape[1]} features\")\n",
    "print(f\"   Validation: {X_val.shape[0]:,} samples\")\n",
    "print(f\"   Test: {X_test.shape[0]:,} samples\")\n",
    "print(f\"   Class balance (train): {(y_train == 0).sum():,} No, {(y_train == 1).sum():,} Yes\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02455f23",
   "metadata": {},
   "source": [
    "## 2. Logistic Regression\n",
    "\n",
    "Logistic Regression is interpretable and widely used in insurance. Note: The data is already scaled from preprocessing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b6eaaf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# MODEL 1: LOGISTIC REGRESSION\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"LOGISTIC REGRESSION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Train model (data is already scaled)\n",
    "logistic_model = LogisticRegression(\n",
    "    class_weight='balanced', \n",
    "    max_iter=1000, \n",
    "    random_state=42,\n",
    "    solver='lbfgs'  # Good default solver\n",
    ")\n",
    "print(\"Training Logistic Regression...\")\n",
    "start_time = time.time()\n",
    "logistic_model.fit(X_train, y_train)\n",
    "training_time = time.time() - start_time\n",
    "print(f\"Training completed in {training_time:.2f} seconds\")\n",
    "\n",
    "# Make predictions on all sets\n",
    "print(\"Generating predictions...\")\n",
    "y_train_pred_lr = logistic_model.predict(X_train)\n",
    "y_train_proba_lr = logistic_model.predict_proba(X_train)[:, 1]\n",
    "y_val_pred_lr = logistic_model.predict(X_val)\n",
    "y_val_proba_lr = logistic_model.predict_proba(X_val)[:, 1]\n",
    "y_test_pred_lr = logistic_model.predict(X_test)\n",
    "y_test_proba_lr = logistic_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Calculate metrics for validation and test sets\n",
    "def calculate_metrics(y_true, y_pred, y_proba, set_name):\n",
    "    \"\"\"\n",
    "    Calculate comprehensive classification metrics.\n",
    "       \n",
    "    Parameters:\n",
    "    -----------\n",
    "    y_true : array-like\n",
    "        True labels\n",
    "    y_pred : array-like\n",
    "        Predicted labels\n",
    "    y_proba : array-like\n",
    "        Predicted probabilities for positive class\n",
    "    set_name : str\n",
    "        Name of the dataset (for tracking)\n",
    "           \n",
    "    Returns:\n",
    "    --------\n",
    "    dict : Dictionary containing accuracy, precision, recall, f1, roc_auc\n",
    "    \"\"\"\n",
    "    return {\n",
    "        'set': set_name,\n",
    "        'accuracy': accuracy_score(y_true, y_pred),\n",
    "        'precision': precision_score(y_true, y_pred, zero_division=0),\n",
    "        'recall': recall_score(y_true, y_pred, zero_division=0),\n",
    "        'f1': f1_score(y_true, y_pred, zero_division=0),\n",
    "        'roc_auc': roc_auc_score(y_true, y_proba) if len(np.unique(y_true)) > 1 else 0.0\n",
    "    }\n",
    "\n",
    "lr_val_metrics = calculate_metrics(y_val, y_val_pred_lr, y_val_proba_lr, 'Validation')\n",
    "lr_test_metrics = calculate_metrics(y_test, y_test_pred_lr, y_test_proba_lr, 'Test')\n",
    "lr_train_metrics = calculate_metrics(y_train, y_train_pred_lr, y_train_proba_lr, 'Train')\n",
    "\n",
    "print(\"\\nüìä Train Set Performance:\")\n",
    "for metric in ['accuracy', 'precision', 'recall', 'f1', 'roc_auc']:\n",
    "    print(f\"   {metric.upper():12s}: {lr_train_metrics[metric]:.4f}\")\n",
    "\n",
    "print(\"\\nüìä Validation Set Performance:\")\n",
    "for metric in ['accuracy', 'precision', 'recall', 'f1', 'roc_auc']:\n",
    "    print(f\"   {metric.upper():12s}: {lr_val_metrics[metric]:.4f}\")\n",
    "\n",
    "print(\"\\nüìä Test Set Performance:\")\n",
    "for metric in ['accuracy', 'precision', 'recall', 'f1', 'roc_auc']:\n",
    "    print(f\"   {metric.upper():12s}: {lr_test_metrics[metric]:.4f}\")\n",
    "\n",
    "# Confusion Matrices\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "for idx, (y_true, y_pred, title) in enumerate([\n",
    "    (y_train, y_train_pred_lr, 'Train Set'),\n",
    "    (y_val, y_val_pred_lr, 'Validation Set'),\n",
    "    (y_test, y_test_pred_lr, 'Test Set')\n",
    "]):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[idx], cbar=False)\n",
    "    axes[idx].set_title(f'Logistic Regression - {title}', fontweight='bold')\n",
    "    axes[idx].set_xlabel('Predicted')\n",
    "    axes[idx].set_ylabel('Actual')\n",
    "    axes[idx].set_xticklabels(['No', 'Yes'])\n",
    "    axes[idx].set_yticklabels(['No', 'Yes'])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ROC Curves\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "fpr_train, tpr_train, _ = roc_curve(y_train, y_train_proba_lr)\n",
    "fpr_val, tpr_val, _ = roc_curve(y_val, y_val_proba_lr)\n",
    "fpr_test, tpr_test, _ = roc_curve(y_test, y_test_proba_lr)\n",
    "\n",
    "ax.plot(fpr_train, tpr_train, 'darkgreen', lw=2, \n",
    "        label=f'Train (AUC={lr_train_metrics[\"roc_auc\"]:.3f})')\n",
    "ax.plot(fpr_val, tpr_val, 'darkorange', lw=2, \n",
    "        label=f'Val (AUC={lr_val_metrics[\"roc_auc\"]:.3f})')\n",
    "ax.plot(fpr_test, tpr_test, 'darkblue', lw=2, \n",
    "        label=f'Test (AUC={lr_test_metrics[\"roc_auc\"]:.3f})')\n",
    "ax.plot([0, 1], [0, 1], 'gray', lw=2, linestyle='--', label='Random')\n",
    "ax.set_xlabel('False Positive Rate')\n",
    "ax.set_ylabel('True Positive Rate')\n",
    "ax.set_title('Logistic Regression - ROC Curves', fontsize=12, fontweight='bold')\n",
    "ax.legend()\n",
    "ax.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úì Logistic Regression complete!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3318117e",
   "metadata": {},
   "source": [
    "## 3. Random Forest\n",
    "\n",
    "Random Forest can capture non-linear relationships and feature interactions. Trees don't require scaling.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5af825b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# MODEL 2: RANDOM FOREST\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"RANDOM FOREST\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Train model\n",
    "rf_model = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=10,\n",
    "    class_weight='balanced',\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "print(\"Training Random Forest (this may take a minute)...\")\n",
    "start_time = time.time()\n",
    "rf_model.fit(X_train, y_train)\n",
    "training_time = time.time() - start_time\n",
    "print(f\"Training completed in {training_time:.2f} seconds\")\n",
    "\n",
    "# Make predictions on all sets\n",
    "print(\"Generating predictions...\")\n",
    "y_train_pred_rf = rf_model.predict(X_train)\n",
    "y_train_proba_rf = rf_model.predict_proba(X_train)[:, 1]\n",
    "y_val_pred_rf = rf_model.predict(X_val)\n",
    "y_val_proba_rf = rf_model.predict_proba(X_val)[:, 1]\n",
    "y_test_pred_rf = rf_model.predict(X_test)\n",
    "y_test_proba_rf = rf_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Calculate metrics\n",
    "rf_val_metrics = calculate_metrics(y_val, y_val_pred_rf, y_val_proba_rf, 'Validation')\n",
    "rf_test_metrics = calculate_metrics(y_test, y_test_pred_rf, y_test_proba_rf, 'Test')\n",
    "rf_train_metrics = calculate_metrics(y_train, y_train_pred_rf, y_train_proba_rf, 'Train')\n",
    "\n",
    "print(\"\\nüìä Train Set Performance:\")\n",
    "for metric in ['accuracy', 'precision', 'recall', 'f1', 'roc_auc']:\n",
    "    print(f\"   {metric.upper():12s}: {rf_train_metrics[metric]:.4f}\")\n",
    "\n",
    "print(\"\\nüìä Validation Set Performance:\")\n",
    "for metric in ['accuracy', 'precision', 'recall', 'f1', 'roc_auc']:\n",
    "    print(f\"   {metric.upper():12s}: {rf_val_metrics[metric]:.4f}\")\n",
    "\n",
    "print(\"\\nüìä Test Set Performance:\")\n",
    "for metric in ['accuracy', 'precision', 'recall', 'f1', 'roc_auc']:\n",
    "    print(f\"   {metric.upper():12s}: {rf_test_metrics[metric]:.4f}\")\n",
    "\n",
    "# Confusion Matrices\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "for idx, (y_true, y_pred, title) in enumerate([\n",
    "    (y_train, y_train_pred_rf, 'Train Set'),\n",
    "    (y_val, y_val_pred_rf, 'Validation Set'),\n",
    "    (y_test, y_test_pred_rf, 'Test Set')\n",
    "]):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Greens', ax=axes[idx], cbar=False)\n",
    "    axes[idx].set_title(f'Random Forest - {title}', fontweight='bold')\n",
    "    axes[idx].set_xlabel('Predicted')\n",
    "    axes[idx].set_ylabel('Actual')\n",
    "    axes[idx].set_xticklabels(['No', 'Yes'])\n",
    "    axes[idx].set_yticklabels(['No', 'Yes'])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ROC Curves\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "fpr_train, tpr_train, _ = roc_curve(y_train, y_train_proba_rf)\n",
    "fpr_val, tpr_val, _ = roc_curve(y_val, y_val_proba_rf)\n",
    "fpr_test, tpr_test, _ = roc_curve(y_test, y_test_proba_rf)\n",
    "\n",
    "ax.plot(fpr_train, tpr_train, 'darkgreen', lw=2, \n",
    "        label=f'Train (AUC={rf_train_metrics[\"roc_auc\"]:.3f})')\n",
    "ax.plot(fpr_val, tpr_val, 'darkorange', lw=2, \n",
    "        label=f'Val (AUC={rf_val_metrics[\"roc_auc\"]:.3f})')\n",
    "ax.plot(fpr_test, tpr_test, 'darkblue', lw=2, \n",
    "        label=f'Test (AUC={rf_test_metrics[\"roc_auc\"]:.3f})')\n",
    "ax.plot([0, 1], [0, 1], 'gray', lw=2, linestyle='--', label='Random')\n",
    "ax.set_xlabel('False Positive Rate')\n",
    "ax.set_ylabel('True Positive Rate')\n",
    "ax.set_title('Random Forest - ROC Curves', fontsize=12, fontweight='bold')\n",
    "ax.legend()\n",
    "ax.grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úì Random Forest complete!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc75e7c7",
   "metadata": {},
   "source": [
    "## 4. Feature Importance Analysis\n",
    "\n",
    "Understanding which features drive the Random Forest predictions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec9809e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# FEATURE IMPORTANCE - RANDOM FOREST\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"FEATURE IMPORTANCE ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Get feature importances\n",
    "if len(feature_names) != len(rf_model.feature_importances_):\n",
    "    raise ValueError(f\"Feature names count ({len(feature_names)}) doesn't match model features ({len(rf_model.feature_importances_)})\")\n",
    "\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': feature_names,\n",
    "    'importance': rf_model.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"\\nüìä Top 15 Most Important Features:\")\n",
    "print(feature_importance.head(15).to_string(index=False))\n",
    "\n",
    "# Visualize top features\n",
    "top_n = 15\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "top_features = feature_importance.head(top_n)\n",
    "\n",
    "ax.barh(range(len(top_features)), top_features['importance'], color='forestgreen', alpha=0.8)\n",
    "ax.set_yticks(range(len(top_features)))\n",
    "ax.set_yticklabels(top_features['feature'])\n",
    "ax.invert_yaxis()\n",
    "ax.set_xlabel('Importance', fontsize=12)\n",
    "ax.set_title(f'Random Forest - Top {top_n} Feature Importances', fontsize=14, fontweight='bold')\n",
    "ax.grid(axis='x', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n‚úì Feature importance analysis complete!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9725100",
   "metadata": {},
   "source": [
    "## 5. Model Comparison\n",
    "\n",
    "Compare both models on train, validation, and test sets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d359c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# MODEL COMPARISON\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"MODEL COMPARISON\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create comprehensive comparison dataframe\n",
    "comparison_data = {\n",
    "    'Model': ['Logistic Regression', 'Logistic Regression', 'Logistic Regression',\n",
    "              'Random Forest', 'Random Forest', 'Random Forest'],\n",
    "    'Set': ['Train', 'Validation', 'Test', 'Train', 'Validation', 'Test'],\n",
    "    'Accuracy': [lr_train_metrics['accuracy'], lr_val_metrics['accuracy'], lr_test_metrics['accuracy'],\n",
    "                 rf_train_metrics['accuracy'], rf_val_metrics['accuracy'], rf_test_metrics['accuracy']],\n",
    "    'Precision': [lr_train_metrics['precision'], lr_val_metrics['precision'], lr_test_metrics['precision'],\n",
    "                  rf_train_metrics['precision'], rf_val_metrics['precision'], rf_test_metrics['precision']],\n",
    "    'Recall': [lr_train_metrics['recall'], lr_val_metrics['recall'], lr_test_metrics['recall'],\n",
    "               rf_train_metrics['recall'], rf_val_metrics['recall'], rf_test_metrics['recall']],\n",
    "    'F1-Score': [lr_train_metrics['f1'], lr_val_metrics['f1'], lr_test_metrics['f1'],\n",
    "                 rf_train_metrics['f1'], rf_val_metrics['f1'], rf_test_metrics['f1']],\n",
    "    'ROC-AUC': [lr_train_metrics['roc_auc'], lr_val_metrics['roc_auc'], lr_test_metrics['roc_auc'],\n",
    "                rf_train_metrics['roc_auc'], rf_val_metrics['roc_auc'], rf_test_metrics['roc_auc']]\n",
    "}\n",
    "\n",
    "comparison = pd.DataFrame(comparison_data)\n",
    "print(\"\\n\", comparison.to_string(index=False))\n",
    "\n",
    "# Visualize comparison\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'ROC-AUC']\n",
    "\n",
    "for idx, metric in enumerate(metrics):\n",
    "    ax = axes[idx // 3, idx % 3]\n",
    "    x = np.arange(3)  # Train, Validation, Test\n",
    "    width = 0.35\n",
    "    \n",
    "    lr_values = [\n",
    "        lr_train_metrics[metric.lower().replace('-', '_')],\n",
    "        lr_val_metrics[metric.lower().replace('-', '_')], \n",
    "        lr_test_metrics[metric.lower().replace('-', '_')]\n",
    "    ]\n",
    "    rf_values = [\n",
    "        rf_train_metrics[metric.lower().replace('-', '_')],\n",
    "        rf_val_metrics[metric.lower().replace('-', '_')], \n",
    "        rf_test_metrics[metric.lower().replace('-', '_')]\n",
    "    ]\n",
    "    \n",
    "    ax.bar(x - width/2, lr_values, width, label='Logistic Regression', \n",
    "           color='darkorange', alpha=0.8)\n",
    "    ax.bar(x + width/2, rf_values, width, label='Random Forest', \n",
    "           color='forestgreen', alpha=0.8)\n",
    "    \n",
    "    ax.set_xlabel('Dataset', fontsize=10)\n",
    "    ax.set_ylabel('Score', fontsize=10)\n",
    "    ax.set_title(metric, fontsize=12, fontweight='bold')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(['Train', 'Validation', 'Test'])\n",
    "    ax.legend()\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "    ax.set_ylim([0, 1.0])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Determine best model based on validation ROC-AUC\n",
    "print(\"\\nüèÜ BEST MODEL (based on Validation ROC-AUC):\")\n",
    "if rf_val_metrics['roc_auc'] > lr_val_metrics['roc_auc']:\n",
    "    print(f\"   Random Forest (Val ROC-AUC: {rf_val_metrics['roc_auc']:.4f})\")\n",
    "    best_model = 'Random Forest'\n",
    "    best_model_obj = rf_model\n",
    "    best_val_pred = y_val_pred_rf\n",
    "    best_test_pred = y_test_pred_rf\n",
    "    best_val_proba = y_val_proba_rf\n",
    "    best_test_proba = y_test_proba_rf\n",
    "else:\n",
    "    print(f\"   Logistic Regression (Val ROC-AUC: {lr_val_metrics['roc_auc']:.4f})\")\n",
    "    best_model = 'Logistic Regression'\n",
    "    best_model_obj = logistic_model\n",
    "    best_val_pred = y_val_pred_lr\n",
    "    best_test_pred = y_test_pred_lr\n",
    "    best_val_proba = y_val_proba_lr\n",
    "    best_test_proba = y_test_proba_lr\n",
    "\n",
    "print(f\"\\n   Test Set Performance:\")\n",
    "if best_model == 'Random Forest':\n",
    "    print(f\"   ROC-AUC: {rf_test_metrics['roc_auc']:.4f}\")\n",
    "    print(f\"   F1-Score: {rf_test_metrics['f1']:.4f}\")\n",
    "else:\n",
    "    print(f\"   ROC-AUC: {lr_test_metrics['roc_auc']:.4f}\")\n",
    "    print(f\"   F1-Score: {lr_test_metrics['f1']:.4f}\")\n",
    "\n",
    "print(\"\\nüìà Summary Statistics:\")\n",
    "print(f\"   Overfitting check (Train vs Val ROC-AUC difference):\")\n",
    "print(f\"   LR: {abs(lr_train_metrics['roc_auc'] - lr_val_metrics['roc_auc']):.4f}\")\n",
    "print(f\"   RF: {abs(rf_train_metrics['roc_auc'] - rf_val_metrics['roc_auc']):.4f}\")\n",
    "\n",
    "lr_overfit = abs(lr_train_metrics['roc_auc'] - lr_val_metrics['roc_auc'])\n",
    "rf_overfit = abs(rf_train_metrics['roc_auc'] - rf_val_metrics['roc_auc'])\n",
    "if lr_overfit > 0.1:\n",
    "    print(f\"   ‚ö†Ô∏è  WARNING: Logistic Regression may be overfitting (gap: {lr_overfit:.4f})\")\n",
    "if rf_overfit > 0.1:\n",
    "    print(f\"   ‚ö†Ô∏è  WARNING: Random Forest may be overfitting (gap: {rf_overfit:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b9142e3",
   "metadata": {},
   "source": [
    "## 6. Save Models and Predictions\n",
    "\n",
    "Save all models, predictions, and metrics for downstream fairness analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e02a0ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# SAVE MODELS AND PREDICTIONS\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"SAVING MODELS AND PREDICTIONS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create results directory\n",
    "results_dir = Path('../results')\n",
    "results_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Save models\n",
    "print(\"Saving models...\")\n",
    "with open(results_dir / 'logistic_regression_model.pkl', 'wb') as f:\n",
    "    pickle.dump(logistic_model, f)\n",
    "    \n",
    "with open(results_dir / 'random_forest_model.pkl', 'wb') as f:\n",
    "    pickle.dump(rf_model, f)\n",
    "\n",
    "print(\"‚úì Models saved\")\n",
    "\n",
    "# Save predictions and probabilities\n",
    "assert len(y_test_pred_lr) == len(y_test), \"LR Prediction length mismatch\"\n",
    "assert len(y_test_proba_lr) == len(y_test), \"LR Probability length mismatch\"\n",
    "assert len(y_test_pred_rf) == len(y_test), \"RF Prediction length mismatch\"\n",
    "assert len(y_test_proba_rf) == len(y_test), \"RF Probability length mismatch\"\n",
    "\n",
    "print(\"Saving predictions...\")\n",
    "\n",
    "# Logistic Regression predictions\n",
    "with open(results_dir / 'lr_train_predictions.pkl', 'wb') as f:\n",
    "    pickle.dump(y_train_pred_lr, f)\n",
    "with open(results_dir / 'lr_val_predictions.pkl', 'wb') as f:\n",
    "    pickle.dump(y_val_pred_lr, f)\n",
    "with open(results_dir / 'lr_test_predictions.pkl', 'wb') as f:\n",
    "    pickle.dump(y_test_pred_lr, f)\n",
    "\n",
    "with open(results_dir / 'lr_train_proba.pkl', 'wb') as f:\n",
    "    pickle.dump(y_train_proba_lr, f)\n",
    "with open(results_dir / 'lr_val_proba.pkl', 'wb') as f:\n",
    "    pickle.dump(y_val_proba_lr, f)\n",
    "with open(results_dir / 'lr_test_proba.pkl', 'wb') as f:\n",
    "    pickle.dump(y_test_proba_lr, f)\n",
    "\n",
    "# Random Forest predictions\n",
    "with open(results_dir / 'rf_train_predictions.pkl', 'wb') as f:\n",
    "    pickle.dump(y_train_pred_rf, f)\n",
    "with open(results_dir / 'rf_val_predictions.pkl', 'wb') as f:\n",
    "    pickle.dump(y_val_pred_rf, f)\n",
    "with open(results_dir / 'rf_test_predictions.pkl', 'wb') as f:\n",
    "    pickle.dump(y_test_pred_rf, f)\n",
    "\n",
    "with open(results_dir / 'rf_train_proba.pkl', 'wb') as f:\n",
    "    pickle.dump(y_train_proba_rf, f)\n",
    "with open(results_dir / 'rf_val_proba.pkl', 'wb') as f:\n",
    "    pickle.dump(y_val_proba_rf, f)\n",
    "with open(results_dir / 'rf_test_proba.pkl', 'wb') as f:\n",
    "    pickle.dump(y_test_proba_rf, f)\n",
    "\n",
    "print(\"‚úì Predictions saved\")\n",
    "\n",
    "# Save metrics\n",
    "print(\"Saving metrics...\")\n",
    "baseline_metrics = {\n",
    "    'logistic_regression': {\n",
    "        'train': {k: float(v) for k, v in lr_train_metrics.items() if k != 'set'},\n",
    "        'validation': {k: float(v) for k, v in lr_val_metrics.items() if k != 'set'},\n",
    "        'test': {k: float(v) for k, v in lr_test_metrics.items() if k != 'set'}\n",
    "    },\n",
    "    'random_forest': {\n",
    "        'train': {k: float(v) for k, v in rf_train_metrics.items() if k != 'set'},\n",
    "        'validation': {k: float(v) for k, v in rf_val_metrics.items() if k != 'set'},\n",
    "        'test': {k: float(v) for k, v in rf_test_metrics.items() if k != 'set'}\n",
    "    },\n",
    "    'best_model': best_model,\n",
    "    'feature_importance': feature_importance.to_dict('records')[:20]  # Top 20 features\n",
    "}\n",
    "\n",
    "with open(results_dir / 'baseline_metrics.json', 'w') as f:\n",
    "    json.dump(baseline_metrics, f, indent=2)\n",
    "\n",
    "print(\"‚úì Metrics saved\")\n",
    "\n",
    "print(f\"\\n‚úÖ All files saved to: {results_dir.absolute()}\")\n",
    "print(\"\\nSaved files:\")\n",
    "print(\"  Models:\")\n",
    "print(\"    - logistic_regression_model.pkl\")\n",
    "print(\"    - random_forest_model.pkl\")\n",
    "print(\"  Predictions (Logistic Regression):\")\n",
    "print(\"    - lr_train_predictions.pkl, lr_val_predictions.pkl, lr_test_predictions.pkl\")\n",
    "print(\"    - lr_train_proba.pkl, lr_val_proba.pkl, lr_test_proba.pkl\")\n",
    "print(\"  Predictions (Random Forest):\")\n",
    "print(\"    - rf_train_predictions.pkl, rf_val_predictions.pkl, rf_test_predictions.pkl\")\n",
    "print(\"    - rf_train_proba.pkl, rf_val_proba.pkl, rf_test_proba.pkl\")\n",
    "print(\"  Metrics:\")\n",
    "print(\"    - baseline_metrics.json\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"NOTEBOOK COMPLETE!\")\n",
    "print(\"=\" * 60)\n",
    "print(\"Next steps:\")\n",
    "print(\"  ‚Üí Proceed to notebook 03_fairness_analysis.ipynb\")\n",
    "print(\"  ‚Üí Load models and predictions from ../results/ directory\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
