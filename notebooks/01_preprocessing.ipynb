{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1aaa6d6c",
   "metadata": {},
   "source": [
    "# 01: Data Preprocessing\n",
    "\n",
    "This notebook handles data preparation for the insurance claim prediction models:\n",
    "- Load and clean the dataset\n",
    "- Handle missing values and outliers\n",
    "- Encode categorical features\n",
    "- Balance the dataset (if needed)\n",
    "- Split into train/validation/test sets\n",
    "- Save processed data for downstream notebooks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c3e4d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies (for Google Colab)\n",
    "!pip install fairlearn seaborn -q\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e6a1a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Create results directory if it doesn't exist\n",
    "Path('../results').mkdir(exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d0b2d9d",
   "metadata": {},
   "source": [
    "## 1. Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b15961e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the raw dataset\n",
    "file_path = '../data/AutoInsurance.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Create a working copy\n",
    "df_processed = df.copy()\n",
    "\n",
    "print(f\"✓ Loaded dataset: {df_processed.shape[0]} rows, {df_processed.shape[1]} columns\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "display(df_processed.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c8d0f1",
   "metadata": {},
   "source": [
    "## 2. Data Quality Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "047ec096",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values and duplicates\n",
    "print(\"=\"*60)\n",
    "print(\"DATA QUALITY CHECKS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Missing values check\n",
    "missing = df_processed.isnull().sum()\n",
    "if missing.sum() > 0:\n",
    "    print(f\"\\n⚠ Missing values found:\")\n",
    "    print(missing[missing > 0])\n",
    "else:\n",
    "    print(\"\\n✓ No missing values found\")\n",
    "\n",
    "# Duplicate check\n",
    "duplicates = df_processed.duplicated().sum()\n",
    "if duplicates > 0:\n",
    "    print(f\"\\n⚠ Warning: {duplicates} duplicate rows found\")\n",
    "    print(\"Removing duplicates...\")\n",
    "    df_processed = df_processed.drop_duplicates()\n",
    "    print(f\"✓ Removed duplicates. New shape: {df_processed.shape}\")\n",
    "else:\n",
    "    print(\"\\n✓ No duplicate rows found\")\n",
    "\n",
    "# Basic data info\n",
    "print(f\"\\n✓ Dataset shape: {df_processed.shape[0]} rows × {df_processed.shape[1]} columns\")\n",
    "print(f\"✓ Data types: {df_processed.dtypes.value_counts().to_dict()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0309859",
   "metadata": {},
   "source": [
    "## 3. Data Transformation\n",
    "### 3.1 Drop Identifier Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65804c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop Customer ID (unique identifier - no predictive value)\n",
    "if 'Customer' in df_processed.columns:\n",
    "    df_processed = df_processed.drop(columns=['Customer'])\n",
    "    print(f\"✓ Dropped 'Customer' column\")\n",
    "    print(f\"Remaining columns: {df_processed.shape[1]}\")\n",
    "else:\n",
    "    print(\"'Customer' column not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3000b2e",
   "metadata": {},
   "source": [
    "### 3.2 Date Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e86039c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'Effective To Date' to datetime and extract features\n",
    "if 'Effective To Date' in df_processed.columns:\n",
    "    df_processed['Effective To Date'] = pd.to_datetime(df_processed['Effective To Date'])\n",
    "    \n",
    "    # Extract temporal features\n",
    "    df_processed['Effective_Year'] = df_processed['Effective To Date'].dt.year\n",
    "    df_processed['Effective_Month'] = df_processed['Effective To Date'].dt.month\n",
    "    df_processed['Effective_DayOfWeek'] = df_processed['Effective To Date'].dt.dayofweek\n",
    "    \n",
    "    # Drop original date column\n",
    "    df_processed = df_processed.drop(columns=['Effective To Date'])\n",
    "    print(\"✓ Extracted temporal features: Year, Month, DayOfWeek\")\n",
    "    print(f\"Date range: {df_processed['Effective_Year'].min()} to {df_processed['Effective_Year'].max()}\")\n",
    "else:\n",
    "    print(\"'Effective To Date' column not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68fd5254",
   "metadata": {},
   "source": [
    "### 3.3 Define and Separate Protected Attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc65acd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define protected attributes (for fairness evaluation)\n",
    "# These are tracked separately for fairness evaluation\n",
    "# Note: They ARE included as model features (see Section 4.3) for comparison purposes\n",
    "protected_attributes_list = [\n",
    "    'Gender',\n",
    "    'EmploymentStatus',\n",
    "    'Education', \n",
    "    'Marital Status',\n",
    "    'Location Code',\n",
    "    'State',\n",
    "    'Income'\n",
    "]\n",
    "\n",
    "# Verify all protected attributes exist\n",
    "missing_protected = [attr for attr in protected_attributes_list if attr not in df_processed.columns]\n",
    "if missing_protected:\n",
    "    print(f\"⚠ Warning: Missing protected attributes: {missing_protected}\")\n",
    "else:\n",
    "    print(\"✓ All protected attributes found in dataset\")\n",
    "    print(f\"Protected attributes: {protected_attributes_list}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce68df1",
   "metadata": {},
   "source": [
    "### 3.4 Target Variable Extraction and Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb3d3946",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract target variable\n",
    "target_col = 'Response'\n",
    "\n",
    "if target_col not in df_processed.columns:\n",
    "    raise ValueError(f\"Target column '{target_col}' not found!\")\n",
    "\n",
    "y = df_processed[target_col].copy()\n",
    "\n",
    "# Check distribution\n",
    "print(\"=\"*60)\n",
    "print(\"TARGET VARIABLE DISTRIBUTION\")\n",
    "print(\"=\"*60)\n",
    "target_dist = pd.Series(y).value_counts()\n",
    "print(target_dist)\n",
    "print(f\"\\nPercentages:\")\n",
    "print(target_dist / len(y) * 100)\n",
    "\n",
    "# Encode target: No=0, Yes=1\n",
    "label_encoder = LabelEncoder()\n",
    "y_encoded = pd.Series(label_encoder.fit_transform(y), name=target_col)\n",
    "\n",
    "print(f\"\\n✓ Encoded target: {dict(zip(label_encoder.classes_, range(len(label_encoder.classes_))))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "378180a8",
   "metadata": {},
   "source": [
    "### 3.5 Feature-Target Separation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a8dc305",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features from target\n",
    "X = df_processed.drop(columns=[target_col]).copy()\n",
    "\n",
    "print(f\"Features shape: {X.shape}\")\n",
    "print(f\"\\nColumn types:\")\n",
    "print(f\"  - Numerical: {X.select_dtypes(include=['int64', 'float64']).shape[1]} columns\")\n",
    "print(f\"  - Categorical: {X.select_dtypes(include=['object', 'category']).shape[1]} columns\")\n",
    "\n",
    "# Identify column types\n",
    "categorical_cols = X.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "numerical_cols = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "\n",
    "print(f\"\\nCategorical columns ({len(categorical_cols)}):\")\n",
    "print(categorical_cols)\n",
    "print(f\"\\nNumerical columns ({len(numerical_cols)}):\")\n",
    "print(numerical_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "462f5100",
   "metadata": {},
   "source": [
    "## 4. Feature Engineering\n",
    "### 4.1 Outlier Capping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52b858ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle outliers for key numerical features\n",
    "# Strategy: Cap extreme values at percentiles\n",
    "\n",
    "outlier_cols = ['Income', 'Total Claim Amount', 'Customer Lifetime Value']\n",
    "outlier_stats = {}\n",
    "\n",
    "for col in outlier_cols:\n",
    "    if col in X.columns:\n",
    "        # Calculate percentiles\n",
    "        p01 = X[col].quantile(0.01)\n",
    "        p99 = X[col].quantile(0.99)\n",
    "        \n",
    "        # Count outliers\n",
    "        outliers_below = (X[col] < p01).sum()\n",
    "        outliers_above = (X[col] > p99).sum()\n",
    "        \n",
    "        # Cap values\n",
    "        X[f'{col}_original'] = X[col].copy()  # Keep original for reference\n",
    "        X[col] = X[col].clip(lower=p01, upper=p99)\n",
    "        \n",
    "        outlier_stats[col] = {\n",
    "            'p01': p01,\n",
    "            'p99': p99,\n",
    "            'outliers_capped_below': outliers_below,\n",
    "            'outliers_capped_above': outliers_above\n",
    "        }\n",
    "        \n",
    "        print(f\"\\n{col}:\")\n",
    "        print(f\"  Capped at [{p01:.2f}, {p99:.2f}]\")\n",
    "        print(f\"  Values capped below: {outliers_below}\")\n",
    "        print(f\"  Values capped above: {outliers_above}\")\n",
    "\n",
    "# Drop original columns (keep only capped versions)\n",
    "for col in outlier_cols:\n",
    "    if f'{col}_original' in X.columns:\n",
    "        X = X.drop(columns=[f'{col}_original'])\n",
    "        \n",
    "print(\"\\n✓ Outlier handling complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0636cfa",
   "metadata": {},
   "source": [
    "### 4.2 Zero-income Indicator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d17da9d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Special handling for Income: ~25% are zeros\n",
    "# Option 1: Create indicator flag for zero income\n",
    "# Option 2: Treat as missing and impute\n",
    "# Option 3: Keep as-is but be aware it's a distinct group\n",
    "\n",
    "# For now, create an indicator flag\n",
    "if 'Income' in X.columns:\n",
    "    X['Income_IsZero'] = (X['Income'] == 0).astype(int)\n",
    "    zero_income_count = X['Income_IsZero'].sum()\n",
    "    zero_income_pct = (zero_income_count / len(X)) * 100\n",
    "    print(f\"Income zeros: {zero_income_count} ({zero_income_pct:.1f}%)\")\n",
    "    print(\"✓ Created 'Income_IsZero' indicator flag\")\n",
    "    \n",
    "    # Note: Income_IsZero is added AFTER categorical/numerical column identification\n",
    "    # This is intentional - the new binary column will be automatically included in\n",
    "    # the encoded feature set during one-hot encoding as it's a numerical (binary) column.\n",
    "    # The column lists (categorical_cols, numerical_cols) identified earlier don't need\n",
    "    # to be updated because they're only used for the encoding step, which processes all\n",
    "    # columns in X at that time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec5645e1",
   "metadata": {},
   "source": [
    "### 4.3 Categorical Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a47d5ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encode categorical features\n",
    "# Note: Using pd.get_dummies for one-hot encoding\n",
    "#\n",
    "# DECISION: drop_first=True eliminates perfect multicollinearity\n",
    "# - What this means: One category per feature is dropped (becomes the reference/baseline)\n",
    "# - Why this matters: Logistic Regression (our baseline model) can have convergence issues \n",
    "#   with perfect multicollinearity (all categories sum to 1)\n",
    "# - Example: For Gender (M/F), if we drop 'M', then 'F'=1 means female, 'F'=0 means male\n",
    "# - Interpretability: Still fully interpretable - the dropped category is the baseline\n",
    "# - Random Forest: Not affected by multicollinearity, so this is fine for both models\n",
    "# - Industry standard: This is the recommended approach in most ML pipelines\n",
    "#\n",
    "# Alternative: drop_first=False creates full dummy sets but can cause issues with linear models\n",
    "\n",
    "# DECISION: Protected attributes are INCLUDED in model features\n",
    "# Rationale for inclusion:\n",
    "#   - Allows comparison of model behavior with/without protected attributes\n",
    "#   - Useful for fairness analysis to see how models perform when they can \"see\" protected groups\n",
    "#   - Can later train separate models excluding these features for comparison experiments\n",
    "#   - Enables studying whether models use protected attributes for prediction\n",
    "# \n",
    "# For production fairness-aware models, you would typically EXCLUDE protected attributes\n",
    "# to avoid direct discrimination. However, for research and comparison purposes, \n",
    "# inclusion is valuable to understand model behavior and demonstrate the need for fairness mitigation.\n",
    "#\n",
    "# Note: Protected attributes are tracked separately for fairness evaluation regardless of\n",
    "# whether they're included as features (see Section 3.3)\n",
    "\n",
    "print(\"Encoding categorical features...\")\n",
    "print(f\"Before encoding: {X.shape}\")\n",
    "\n",
    "# One-hot encode using pandas get_dummies\n",
    "X_encoded = pd.get_dummies(\n",
    "    X, \n",
    "    columns=categorical_cols, \n",
    "    drop_first=True,  # Remove one category per feature (eliminates perfect multicollinearity)\n",
    "    dtype=int\n",
    ")\n",
    "\n",
    "print(f\"After encoding: {X_encoded.shape}\")\n",
    "print(f\"New columns created: {X_encoded.shape[1] - len(numerical_cols)}\")\n",
    "print(\"\\n✓ Categorical encoding complete (reference categories dropped to avoid multicollinearity)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8060c5c",
   "metadata": {},
   "source": [
    "## 5. Data Splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a2ee78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create stratified splits (70% train, 15% validation, 15% test)\n",
    "# Stratified split maintains class distribution\n",
    "\n",
    "# IMPORTANT: Split features first, then use the same indices for protected attributes\n",
    "# to ensure perfect alignment\n",
    "\n",
    "# First split: train+val (85%) vs test (15%)\n",
    "X_temp, X_test, y_temp, y_test = train_test_split(\n",
    "    X_encoded, \n",
    "    y_encoded,\n",
    "    test_size=0.15,\n",
    "    random_state=42,\n",
    "    stratify=y_encoded\n",
    ")\n",
    "\n",
    "# Get indices for alignment (NEW - this captures which rows went where)\n",
    "train_val_indices = X_temp.index\n",
    "test_indices = X_test.index\n",
    "\n",
    "# Second split: train (70%) vs val (15%)\n",
    "# Calculate split size: 0.15 / 0.85 ≈ 0.176\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_temp,\n",
    "    y_temp,\n",
    "    test_size=0.176,  # This gives us ~15% of total\n",
    "    random_state=42,\n",
    "    stratify=y_temp\n",
    ")\n",
    "\n",
    "# Get indices for alignment (NEW - this captures which rows went where)\n",
    "train_indices = X_train.index\n",
    "val_indices = X_val.index\n",
    "\n",
    "# Split protected attributes using the SAME indices as features\n",
    "# This ensures perfect alignment between features and protected attributes\n",
    "protected_df = df_processed[protected_attributes_list].copy()\n",
    "\n",
    "# Ensure protected_df has the same index as df_processed\n",
    "if not protected_df.index.equals(df_processed.index):\n",
    "    protected_df.index = df_processed.index\n",
    "\n",
    "# Split protected attributes using the indices from feature splits\n",
    "# This uses .loc[] to select rows by index instead of splitting randomly\n",
    "protected_temp = protected_df.loc[train_val_indices]\n",
    "protected_test = protected_df.loc[test_indices]\n",
    "\n",
    "protected_train = protected_df.loc[train_indices]\n",
    "protected_val = protected_df.loc[val_indices]\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"DATA SPLIT SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Training set:   {X_train.shape[0]:,} samples ({X_train.shape[0]/len(X_encoded)*100:.1f}%)\")\n",
    "print(f\"Validation set: {X_val.shape[0]:,} samples ({X_val.shape[0]/len(X_encoded)*100:.1f}%)\")\n",
    "print(f\"Test set:       {X_test.shape[0]:,} samples ({X_test.shape[0]/len(X_encoded)*100:.1f}%)\")\n",
    "print(f\"\\nTotal features: {X_train.shape[1]}\")\n",
    "\n",
    "# Verify split alignment (NEW - this checks that everything matches)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"VERIFYING SPLIT ALIGNMENT\")\n",
    "print(\"=\"*60)\n",
    "assert len(protected_train) == len(X_train), f\"Train split mismatch! {len(protected_train)} != {len(X_train)}\"\n",
    "assert len(protected_val) == len(X_val), f\"Validation split mismatch! {len(protected_val)} != {len(X_val)}\"\n",
    "assert len(protected_test) == len(X_test), f\"Test split mismatch! {len(protected_test)} != {len(X_test)}\"\n",
    "print(\"✓ All splits aligned correctly\")\n",
    "\n",
    "# Check target distribution in each split\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TARGET DISTRIBUTION BY SPLIT\")\n",
    "print(\"=\"*60)\n",
    "for split_name, y_split in [('Train', y_train), ('Validation', y_val), ('Test', y_test)]:\n",
    "    dist = pd.Series(y_split).value_counts(normalize=True) * 100\n",
    "    print(f\"\\n{split_name}:\")\n",
    "    for label, pct in dist.items():\n",
    "        label_name = label_encoder.inverse_transform([label])[0]\n",
    "        print(f\"  {label_name}: {pct:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a0e15a6",
   "metadata": {},
   "source": [
    "## 6 Scaling & Preparation\n",
    "### 6.1 Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4815c574",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale features (fit on training data only)\n",
    "# NOTE: We scale ALL features (including one-hot encoded) for consistency\n",
    "# While one-hot encoded features don't strictly need scaling, it ensures\n",
    "# all features are on the same scale for algorithms sensitive to feature magnitude\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit scaler on training data\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_train_scaled = pd.DataFrame(X_train_scaled, columns=X_train.columns, index=X_train.index)\n",
    "\n",
    "# Transform validation and test sets\n",
    "X_val_scaled = pd.DataFrame(\n",
    "    scaler.transform(X_val), \n",
    "    columns=X_val.columns, \n",
    "    index=X_val.index\n",
    ")\n",
    "X_test_scaled = pd.DataFrame(\n",
    "    scaler.transform(X_test), \n",
    "    columns=X_test.columns, \n",
    "    index=X_test.index\n",
    ")\n",
    "\n",
    "print(\"✓ Feature scaling complete (all features scaled for consistency)\")\n",
    "print(f\"Scaled training set mean: {X_train_scaled.mean().mean():.6f}\")\n",
    "print(f\"Scaled training set std: {X_train_scaled.std().mean():.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "994528cd",
   "metadata": {},
   "source": [
    "### 6.2 Addressing Class Imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce94b435",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class imbalance: ~85.7% No vs 14.3% Yes (ratio: 5.98:1)\n",
    "# Options:\n",
    "# 1. Use SMOTE for oversampling (applied later in training)\n",
    "# 2. Use class weights in models\n",
    "# 3. Keep as-is\n",
    "\n",
    "# For now, calculate class weights for use in models\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "class_weights = compute_class_weight(\n",
    "    'balanced',\n",
    "    classes=np.unique(y_train),\n",
    "    y=y_train\n",
    ")\n",
    "class_weight_dict = dict(zip(np.unique(y_train), class_weights))\n",
    "\n",
    "print(\"Class weights for balanced training:\")\n",
    "for class_idx, weight in class_weight_dict.items():\n",
    "    class_name = label_encoder.inverse_transform([class_idx])[0]\n",
    "    print(f\"  {class_name} (class {class_idx}): {weight:.3f}\")\n",
    "\n",
    "print(\"\\n⚠ Note: Actual resampling (SMOTE) can be applied during model training\")\n",
    "print(\"For now, we'll save the data as-is and handle imbalance in training notebooks\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c40863c6",
   "metadata": {},
   "source": [
    "## 7. Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcccf34b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all processed data to results directory\n",
    "\n",
    "# Final verification before saving\n",
    "print(\"=\"*60)\n",
    "print(\"FINAL VERIFICATION BEFORE SAVING\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Verify split alignment one more time\n",
    "assert len(protected_train) == len(X_train), f\"Train split mismatch! {len(protected_train)} != {len(X_train)}\"\n",
    "assert len(protected_val) == len(X_val), f\"Validation split mismatch! {len(protected_val)} != {len(X_val)}\"\n",
    "assert len(protected_test) == len(X_test), f\"Test split mismatch! {len(protected_test)} != {len(X_test)}\"\n",
    "\n",
    "# Verify target alignment\n",
    "assert len(y_train) == len(X_train), \"y_train length mismatch with X_train\"\n",
    "assert len(y_val) == len(X_val), \"y_val length mismatch with X_val\"\n",
    "assert len(y_test) == len(X_test), \"y_test length mismatch with X_test\"\n",
    "\n",
    "# Verify protected attributes shape\n",
    "assert protected_train.shape[1] == len(protected_attributes_list), \"Protected attributes column mismatch\"\n",
    "\n",
    "print(\"✓ All data structures verified and aligned\")\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "import json\n",
    "\n",
    "results_dir = Path('../results')\n",
    "results_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# Save feature matrices (scaled)\n",
    "with open(results_dir / 'X_train.pkl', 'wb') as f:\n",
    "    pickle.dump(X_train_scaled, f)\n",
    "with open(results_dir / 'X_val.pkl', 'wb') as f:\n",
    "    pickle.dump(X_val_scaled, f)\n",
    "with open(results_dir / 'X_test.pkl', 'wb') as f:\n",
    "    pickle.dump(X_test_scaled, f)\n",
    "\n",
    "# Save target variables\n",
    "with open(results_dir / 'y_train.pkl', 'wb') as f:\n",
    "    pickle.dump(y_train, f)\n",
    "with open(results_dir / 'y_val.pkl', 'wb') as f:\n",
    "    pickle.dump(y_val, f)\n",
    "with open(results_dir / 'y_test.pkl', 'wb') as f:\n",
    "    pickle.dump(y_test, f)\n",
    "\n",
    "# Save protected attributes\n",
    "with open(results_dir / 'protected_train.pkl', 'wb') as f:\n",
    "    pickle.dump(protected_train, f)\n",
    "with open(results_dir / 'protected_val.pkl', 'wb') as f:\n",
    "    pickle.dump(protected_val, f)\n",
    "with open(results_dir / 'protected_test.pkl', 'wb') as f:\n",
    "    pickle.dump(protected_test, f)\n",
    "\n",
    "# Save scaler\n",
    "with open(results_dir / 'scaler.pkl', 'wb') as f:\n",
    "    pickle.dump(scaler, f)\n",
    "\n",
    "# Save feature names\n",
    "with open(results_dir / 'feature_names.pkl', 'wb') as f:\n",
    "    pickle.dump(X_train_scaled.columns.tolist(), f)\n",
    "\n",
    "# Save label encoder\n",
    "with open(results_dir / 'label_encoder.pkl', 'wb') as f:\n",
    "    pickle.dump(label_encoder, f)\n",
    "\n",
    "# Save class weights\n",
    "with open(results_dir / 'class_weights.pkl', 'wb') as f:\n",
    "    pickle.dump(class_weight_dict, f)\n",
    "\n",
    "# Save preprocessing metadata\n",
    "metadata = {\n",
    "    'preprocessing_date': pd.Timestamp.now().isoformat(),\n",
    "    'original_shape': df.shape,\n",
    "    'processed_shape': X_train_scaled.shape,\n",
    "    'target_column': target_col,\n",
    "    'protected_attributes': protected_attributes_list,\n",
    "    'categorical_columns': categorical_cols,\n",
    "    'numerical_columns': numerical_cols,\n",
    "    'outlier_handling': 'Capped at 1st and 99th percentiles',\n",
    "    'outlier_stats': {k: {str(ki): str(vi) for ki, vi in v.items()} for k, v in outlier_stats.items()},\n",
    "    'class_imbalance_ratio': len(y_train[y_train==0]) / len(y_train[y_train==1]),\n",
    "    'train_size': len(X_train),\n",
    "    'val_size': len(X_val),\n",
    "    'test_size': len(X_test),\n",
    "    'features_created': list(X_train_scaled.columns)\n",
    "}\n",
    "\n",
    "with open(results_dir / 'preprocessing_metadata.json', 'w') as f:\n",
    "    json.dump(metadata, f, indent=2, default=str)\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"SAVED PROCESSED DATA\")\n",
    "print(\"=\"*60)\n",
    "print(f\"✓ Feature matrices: X_train.pkl, X_val.pkl, X_test.pkl\")\n",
    "print(f\"✓ Target variables: y_train.pkl, y_val.pkl, y_test.pkl\")\n",
    "print(f\"✓ Protected attributes: protected_train.pkl, protected_val.pkl, protected_test.pkl\")\n",
    "print(f\"✓ Scaler: scaler.pkl\")\n",
    "print(f\"✓ Feature names: feature_names.pkl\")\n",
    "print(f\"✓ Label encoder: label_encoder.pkl\")\n",
    "print(f\"✓ Class weights: class_weights.pkl\")\n",
    "print(f\"✓ Metadata: preprocessing_metadata.json\")\n",
    "print(f\"\\nAll files saved to: {results_dir.absolute()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "864208de",
   "metadata": {},
   "source": [
    "## 8. Preprocessing Summary & Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc5e64a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final verification and summary\n",
    "print(\"=\"*60)\n",
    "print(\"PREPROCESSING SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\n1. Data Shape:\")\n",
    "print(f\"   Original: {df.shape}\")\n",
    "print(f\"   Processed: {X_train_scaled.shape[1]} features\")\n",
    "\n",
    "print(f\"\\n2. Transformations Applied:\")\n",
    "print(f\"   ✓ Dropped 'Customer' identifier column\")\n",
    "print(f\"   ✓ Converted 'Effective To Date' to temporal features\")\n",
    "print(f\"   ✓ Handled outliers (capped at percentiles)\")\n",
    "print(f\"   ✓ Created 'Income_IsZero' indicator\")\n",
    "print(f\"   ✓ Encoded {len(categorical_cols)} categorical features\")\n",
    "print(f\"   ✓ Scaled all features (StandardScaler)\")\n",
    "\n",
    "print(f\"\\n3. Data Splits:\")\n",
    "print(f\"   Train: {len(X_train):,} samples\")\n",
    "print(f\"   Validation: {len(X_val):,} samples\")\n",
    "print(f\"   Test: {len(X_test):,} samples\")\n",
    "\n",
    "print(f\"\\n4. Protected Attributes Tracked:\")\n",
    "for attr in protected_attributes_list:\n",
    "    print(f\"   - {attr}\")\n",
    "\n",
    "print(f\"\\n5. Next Steps:\")\n",
    "print(f\"   → Proceed to notebook 02_baseline_models.ipynb\")\n",
    "print(f\"   → Load saved data from ../results/ directory\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
