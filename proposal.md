Insurance companies are increasing their use of machine learning to predict claim likelihood or set premium rates. These models are employed to analyze historical data and policyholder characteristics, allowing insurers to make informed decisions and improve the accuracy of their pricing and claims management processes. However, there are risks of models perpetuating demographic or socioeconomic biases that may unfairly disadvantage certain groups (e.g., by gender, age or income). Our project will test how fair and accurate ML-based insurance claim prediction models are when trained on real-world demographic and behavioral data. We hypothesize that models optimizing purely for predictive accuracy will exhibit disparities across demographic groups, and that applying fairness-aware modeling techniques can reduce these biases without severely degrading performance.

This project aligns with class discussions on fairness and transparency in data-driven decision systems. We will build on literature including Barocas et al. (2019) Fairness and Machine Learning, Mehrabi et al. (2021) A Survey on Bias and Fairness in Machine Learning, and Corbett-Davies & Goel (2018) The Measure and Mismeasure of Fairness. These works provide grounding for fairness metrics and interventions, directly connecting to themes explored in lecture and readings.

We will use the publicly available vehicle insurance customer data from Kaggle, which includes demographic and behavioral variables such as age, gender, income, vehicle type, and claim history. Our workflow will start with data preparation, where we will clean, encode categorical features and balance the dataset. We will then train baseline models (Logistic Regression, Random Forest) for claim prediction. Models will be evaluated on accuracy using strategies such as ROC-AUC and F1 scores. For fairness we will use approaches such as demographic parity, equalized odds, and precision parity. Afterwards we will apply fairness-aware techniques (e.g. reweighting or post-processing adjustments) and compare tradeoffs between accuracy and fairness.

We expect that high-performing models demonstrate group-level bias, and that introducing fairness constraints is likely to reduce these disparities at some cost to predictive performance. This experiment will provide empirical evidence for fairnessâ€“utility tradeoffs in automated decision systems.

Bias in automated insurance systems can lead to discriminatory pricing and reinforce systemic inequities. Our analysis aims to highlight these risks and demonstrate practical steps for mitigating them through transparent modeling practices. The broader goal is to contribute to the development of equitable, accountable, and socially responsible predictive systems.